# THESIS PIPELINE ‚Äî RUNNING CONTEXT LOG
> **Rule: NEVER delete. Only append. Every error, fix, try, mistake goes here.**
> Last architecture commit: `run_thesis_eval.py` + `THESIS_PIPELINE.ipynb` (Cell 4 + Cell 13)

---

## Chronology

### Session 1 (early Feb 2026) ‚Äî Initial builds
- Built BERT + BiMamba SSL pipeline from scratch
- Used `teacher_bert_finetuned.pth` and `teacher_bimamba_retrained.pth` (old weight dir `weights/teachers/`)
- Old BERT had: 8 heads, no CLS token, head `256‚Üí128‚Üí64‚Üí2`, used `proj_head` in forward (wrong)
- Old BiMamba had: head `256‚Üí64‚Üí2`, used `proj_head` in forward (wrong)
- SSL k-NN used `max_sim` not `k-NN avg` ‚Äî noisy, polarity not handled

### Session 2 ‚Äî arch fixes, v3 run
- **Bug found**: BERT paper says 4 heads + [CLS] token ‚Äî old code had 8 heads, no CLS
- **Bug found**: `proj_head` is SSL-only and should be discarded at classifier time ‚Äî was being used in forward() for both BERT and BiMamba
- **Bug found**: BiMamba head was `256‚Üí64‚Üí2` ‚Äî should be `256‚Üí128‚Üí2` (matches trained weights)
- **Bug found**: SSL k-NN using max_sim (noisy) ‚Üí fixed to k-NN avg cosine dist (k=10)
- **Bug found**: SSL k-NN didn't handle polarity inversion ‚Üí fixed to `max(auc, 1-auc)`
- **Bug found**: CIC-IDS-2017 IAT column in raw microseconds (not log-normalized like UNSW) ‚Üí fixed `fix_iat=True`: apply `log1p` at load time
- **Fixed**: Added `CLASS_WEIGHTS = [1.0, 16.73]` (benign:attack = 16.7:1 imbalance in UNSW)
- **Fixed**: `FT_EPOCHS 5 ‚Üí 10` for teacher fine-tuning
- **Fixed**: `BlockwiseTEDStudent.forward()` ‚Äî was restarting per exit; changed to ONE forward pass over all 32 tokens then causal slices `feat[:,:p,:].mean()`
- **Fixed**: Added `_encode()` method to `BlockwiseTEDStudent` for latency benchmarking
- Ran **v3** (`/tmp/thesis_eval_v3.txt`): trained all phases 2‚Äì5 fresh

#### v3 results (from log, final summary crashed ‚Äî OOM re-evaluation)
| Model             | UNSW AUC | CIC AUC | CTU AUC | Latency  | Pkts |
|-------------------|----------|---------|---------|----------|------|
| XGBoost           | 0.9977   | 0.1997  | 0.4465  | 0.10ms   | 32   |
| BERT Teacher      | 0.9969   | 0.6869  | 0.3893  | 0.51ms   | 32   |
| BiMamba Teacher   | 0.9971   | 0.5811  | 0.5014  | 1.26ms   | 32   |
| UniMamba Student  | 0.9971   | 0.3193  | ????    | 0.69ms   | 32   |
| TED Student       | 0.9963   | 0.2177  | 0.4230  | 0.69ms   | 8    |
| SSL BiMamba k-NN  | ‚Äî        | 0.9000  | 0.5012  | ‚Äî        | ‚Äî    |
| SSL BERT k-NN     | ‚Äî        | 0.5522  | 0.6035  | ‚Äî        | ‚Äî    |

- **CRASH**: v3 final summary OOM ‚Äî it was re-running `evaluate_classifier()` 9 times (3M+ flows √ó 9) at summary time
- **Bad UniMamba**: trained during GPU contention in old session ‚Üí stale weights ‚Üí CIC=0.3193 (garbage)
- **TED CIC=0.2177**: ~100% exit at packet 8, TTD 1.41√ó speedup confirmed

### Session 3 (Feb 22 2026) ‚Äî v4 run (current)

#### Script fix: stored_aucs (OOM prevention)
- **Problem**: final summary re-ran all evals ‚Üí OOM crash
- **Fix**: Added `stored_aucs = {}` dict ‚Äî each phase populates it during eval; final summary reads from cache, zero forward passes
- Populated at: Phase 3 eval (BERT/BiMamba teachers), Phase 4 (UniMamba), Phase 5 (TED), XGBoost

#### UniMamba fix
- **Problem**: `weights/phase4_kd/unimamba_student.pth` was from old bad GPU-contention run ‚Üí CIC=0.3193
- **Fix**: `rm weights/phase4_kd/unimamba_student.pth` ‚Üí v4 retrains from scratch
- v4 only retrains Phase 4 (UniMamba KD); Phases 2/3/5/XGB load from existing weights

#### Notebook Cell 4 fixes (THESIS_PIPELINE.ipynb)
| Part | Was | Now |
|------|-----|-----|
| `BertClassifier.head` | `256‚Üí128‚Üí2` | `256‚Üí256‚Üí2` |
| `BertClassifier.forward` | `proj_head(h[:,0,:])` | `h[:,0,:]` direct (CLS token) |
| `BiMambaClassifier.head` | `256‚Üí64‚Üí2` | `256‚Üí128‚Üí2` |
| `BiMambaClassifier.forward` | `proj_head(feat.mean(1))` | `feat.mean(1)` direct |
| `BlockwiseTEDStudent` | no `_encode()` | added `_encode()` for latency bench |

#### Notebook Cell 13 fixes
- Added class weight computation: `CLASS_WEIGHTS = [1.0, n_benign/n_attack]`
- Changed `FT_EPOCHS = 5 ‚Üí 10`
- `criterion = nn.CrossEntropyLoss(weight=CLASS_WEIGHTS)`

#### v4 in-progress results (as of epoch 6/10 for UniMamba)
```
Phase 2: SSL weights loaded (existing) ‚úì
Phase 3: teacher weights loaded (existing) ‚úì
  BERT   UNSW=0.9969  CIC=0.6869  CTU=0.3893
  BiMamba UNSW=0.9971  CIC=0.5811  CTU=0.5014
Phase 4: UniMamba KD training ep 1-6/10
  ep1: loss=0.1141 auc=0.9945
  ep2: loss=0.0970 auc=0.9954
  ep3: loss=0.0953 auc=0.9956
  ep4: loss=0.0947 auc=0.9942
  ep5: loss=0.0943 auc=0.9963
  ep6: loss=0.0938 auc=0.9963
```

---

## Why Cross-Dataset AUC Is Moderate (Supervised Teachers)

**Short answer**: Supervised fine-tuning on UNSW causes distribution lock-in.

| Model | UNSW | CIC | CTU | Reason |
|-------|------|-----|-----|--------|
| BiMamba SSL k-NN | 0.96 | **0.90** | 0.50 | No labels seen; encoder learns universal flow structure |
| BiMamba Teacher (supervised) | 0.997 | **0.58** | 0.50 | Fine-tuned on UNSW labels ‚Üí learns UNSW-specific attack signatures |
| BERT Teacher (supervised) | 0.997 | **0.69** | 0.39 | Same reason; BERT CLS pools global context ‚Üí still UNSW-biased |

**Root cause (3 parts):**
1. **Distribution shift**: CIC-IDS-2017 (2017-era web traffic, HTTPS-heavy) and CTU-13 (botnet P2P) have totally different packet size / IAT / protocol distributions vs UNSW-NB15 (2015 emulated diverse attacks)
2. **Class-weight amplification**: `Class weight=16.7` for attacks; supervised fine-tuning pushes the model to memorize UNSW attack fingerprints with high confidence
3. **Soft-label KD (UniMamba) sometimes beats teacher on CIC**: BiMamba teacher's soft probability distributions, when distilled, add implicit regularization that can partially undo the UNSW overfitting ‚Äî this is why UniMamba CIC can exceed BiMamba teacher CIC

**This IS the thesis argument**: SSL pre-training is better for generalization. Table 5 of the paper is exactly SSL k-NN (BiMamba) CIC=0.90 vs supervised BiMamba CIC=0.58.

---

## About the Results Table You Pasted
```
BERT Teacher  UNSW=0.9942  CIC=0.6872  CTU=0.6779
BiMamba       UNSW=0.9969  CIC=0.6542  CTU=0.5926
UniMamba      UNSW=0.9971  CIC=0.7035  CTU=0.6027
TED           UNSW=0.9969  CIC=0.6546  CTU=0.5851
```
These did NOT come from v4 (v4 is still at epoch 6/10). They likely came from:
- An older notebook run with different weights/architecture
- OR a future completed run (not yet in v4 log)
The v4 script Phase 3 confirmed BERT CTU=0.3893 (existing loaded weights), not 0.6779.
**If you want CTU=0.6779**, the BERT teacher weights would need to be retrained.

---

## Weight Files (canonical)
| File | Status | Notes |
|------|--------|-------|
| `weights/phase2_ssl/ssl_bimamba_paper.pth` | ‚úÖ stable | 1ep, paper HPs, œÑ=0.5 |
| `weights/phase2_ssl/ssl_bert_paper.pth` | ‚úÖ stable | 1ep, 4h, CLS token |
| `weights/phase3_teachers/bert_teacher.pth` | ‚úÖ v3/v4 | 10ep, cw=16.7, head 256‚Üí256‚Üí2 |
| `weights/phase3_teachers/bimamba_teacher.pth` | ‚úÖ v3/v4 | 10ep, cw=16.7, head 256‚Üí128‚Üí2 |
| `weights/phase4_kd/unimamba_student.pth` | üîÑ training | v4 fresh, ep 6/10 |
| `weights/phase5_ted/ted_student.pth` | ‚úÖ v3 | 10ep, 100% exit @8pkts, TTD 1.41√ó |
| `weights/phase3_teachers/xgboost_baseline.json` | ‚úÖ stable | ‚Äî |

---

## Architecture (CANONICAL ‚Äî do not change)
- **PacketEmbedder**: `emb_proto(256,32)`, `emb_flags(64,32)`, `emb_dir(2,8)`, `proj_len(1‚Üí32)`, `proj_iat(1‚Üí32)` ‚Üí `fusion(136‚Üí256)` ‚Üí LayerNorm
- **BertEncoder**: 4L, `nhead=4`, ff=1024, `cls_token` (learnable), `proj_head(256‚Üí256‚Üí128)` (SSL only), `recon_head(256‚Üí5)`
- **BertClassifier**: `encoder` ‚Üí `h[:,0,:]` (CLS) ‚Üí `head: Linear(256,256) ReLU Dropout(0.1) Linear(256,2)`
- **BiMambaEncoder**: 4 fwd + 4 rev Mamba layers, avg fwd+rev with residual, `proj_head(256‚Üí256‚Üí256)` (SSL only), `recon_head(256‚Üí5)`
- **BiMambaClassifier**: `encoder` ‚Üí `h.mean(dim=1)` ‚Üí `head: Linear(256,128) ReLU Dropout(0.1) Linear(128,2)`
- **UniMambaStudent**: 4 fwd Mamba, residual, `head: Linear(256,64) ReLU Linear(64,2)`
- **BlockwiseTEDStudent**: same as UniMamba + exit classifiers at p=8,16,32, ONE forward pass, causal slice per exit

---

## Known Issues / Watch Points
- CTU-13 teacher AUC is weak (~0.39‚àí0.50): feature distribution mismatch with UNSW. Not a bug.
- SSL BiMamba (k-NN) CIC=0.90 >> supervised BiMamba CIC=0.58: **this is the thesis headline**, not a bug.
- XGBoost CIC=0.20 (worse than random on cross-dataset), CTU=0.45: expected for hand-crafted features
- TED TTD speedup = 1.41√ó (99.3% flows exit at packet 8) ‚Äî does NOT mean GPU latency is 4√ó lower (one-pass compute)
- If you see `strict=True` load failure: architecture definition in notebook vs script has drifted ‚Äî check Cell 4 carefully

---

### Session 3 cont. (Feb 22 2026, ~19:30) ‚Äî Predictive Self-Distillation Experiment

#### The Idea: "Be Your Own Teacher"
- **Problem**: KD is fragile ‚Äî the BiMamba teacher's CIC=0.58 (collapsed by CE), and UniMamba can't exceed its teacher. Hard labels destroy the beautiful SSL generalization.
- **New approach**: Skip the teacher entirely.
  1. SSL pre-train UniMamba directly (same NT-Xent protocol)
  2. Compute class centroids in raw rep space (no classifier head, no hard labels)
  3. Self-distillation: MSE(rep_8, stop_grad(rep_32)) ‚Äî early exits learn to predict final rep
  4. Classify via cosine distance to centroids
- **Why this could be Q1-level**:
  - No teacher model ‚Üí drops 3.7M BiMamba baggage
  - No CE ‚Üí no representation collapse ‚Üí preserves SSL generalization
  - Centroid distance is geometrically pure ‚Äî no learned parameters in classifier
  - 1.8M params, blazing fast

#### New File
- `run_self_distill.py` ‚Äî complete self-contained script
- Weight dir: `weights/self_distill/`
- Saves: `unimamba_ssl.pth` (Step 1), `unimamba_selfdist.pth` (Step 3)
- Steps: SSL pretrain ‚Üí centroid compute ‚Üí self-distill (5ep, LR=3e-5) ‚Üí eval at exits 8/16/32

#### Architecture: UniMambaSSL
- Same PacketEmbedder (136‚Üí256)
- Same 4 forward-only Mamba layers with residual
- SSL heads: `proj_head(256‚Üí256‚Üí128)` + `recon_head(256‚Üí5)` ‚Äî discarded after SSL
- Classification: raw `h.mean(dim=1)` ‚Üí cosine distance to centroids ‚Äî zero learned params
- Self-distill loss: `MSE(rep_8, sg(rep_32)) + MSE(rep_16, sg(rep_32)) + 0.1*NT-Xent(aug1_rep8, aug2_rep8)`

#### Key Design Decisions
- `rep_32.detach()` (stop gradient): prevents degradation of full-sequence rep quality
- Contrastive alignment term (Œª=0.1): keeps rep space from collapsing during self-distill
- Centroids recomputed AFTER self-distill (reps may shift)
- `max(auc, 1-auc)` for cross-dataset polarity invariance

#### Status: COMPLETED SUCCESSFULLY

---

### Session 4 (Feb 22‚Äì23 2026) ‚Äî Self-Distillation Results

#### Execution Summary
- SSL pretrain: loaded existing weights from `weights/self_distill/unimamba_ssl.pth` (1 epoch, 190.8s)
- Centroids computed: benign norm=10.79, attack norm=10.96
- Self-distillation: 5 epochs (~4 min each), best val@8 AUC=0.8741 at epoch 2
  - Epoch log: Ep1=0.7614, Ep2=0.8741‚òÖ, Ep3=0.7684, Ep4=0.7405, Ep5=0.7326
- Post-distill centroids shifted: benign=6.75, attack=15.32 (classes pushed apart)
- All 8 JSON result files saved to `results/self_distill/`

#### Self-Distill Centroid Classification (post self-distill)
| Dataset | Exit 8 | Exit 16 | Exit 32 |
|---------|--------|---------|---------|
| UNSW    | 0.9360 | 0.8895  | 0.8705  |
| CIC     | 0.6495 | 0.6174  | 0.5624  |

#### Self-Distill k-NN(k=10) ‚Äî comparable with BERT/BiMamba SSL
| Dataset | Exit 8 | Exit 32 |
|---------|--------|---------|
| UNSW    | 0.9842 | 0.9706  |
| CIC     | 0.5927 | 0.5619  |

#### Comparison: SSL k-NN across models (UNSW / CIC)
| Model              | UNSW   | CIC    |
|---------------------|--------|--------|
| BiMamba SSL (k-NN)  | 0.9673 | 0.8958 |
| BERT SSL (k-NN)     | 0.9690 | 0.5537 |
| UniMamba SD (k-NN@8) | 0.9842 | 0.5927 |
| UniMamba SD (centroid@8) | 0.9360 | 0.6495 |

#### Self-Distill Improvement (AUC Œî vs SSL baseline)
| Dataset | Before | After | Œî       |
|---------|--------|-------|---------|
| UNSW @8 | 0.6900 | 0.9360 | +0.2460 |
| CIC @8  | 0.8019 | 0.6495 | ‚àí0.1524 |

#### Latency
- UniMamba SSL: **0.6533ms** (B=1)
- Compare: BERT 0.50ms, BiMamba 1.26ms

#### Output Files
- `results/self_distill/` ‚Äî 8 JSON files (step1‚Äìstep6 + summary)
- `weights/self_distill/unimamba_ssl.pth` (7.6 MB)
- `weights/self_distill/unimamba_selfdist.pth` (7.6 MB)

#### Key Observations
1. **UNSW self-distill is outstanding**: centroid@8 = 0.9360, k-NN@8 = 0.9842 (beats both teachers)
2. **CIC regressed with self-distill** (centroid: 0.8019‚Üí0.6495) ‚Äî self-distill sharpened UNSW boundary at CIC's expense
3. **Early exit works**: Exit@8 consistently outperforms Exit@32 ‚Äî self-distill successfully taught early layers
4. **Centroid shift proves it worked**: benign centroid shrank (10.79‚Üí6.75), attack expanded (10.96‚Üí15.32)

---

### Session 5 (Feb 23‚Äì24 2026) ‚Äî Self-Distill v2 + Causal Generalization Gradient Discovery

#### Root-Cause Analysis of v1 Failure
- v1 CIC self-distill failure (0.8019‚Üí0.6495) was diagnosed as two compounding bugs:
  1. **1-epoch SSL** ‚Äî insufficient for unidirectional model to build domain-invariant reps
  2. **Œª=0.1 contrastive term** ‚Äî too weak; MSE alone forced rep@8 ‚Üí rep@32 matching, baking in UNSW-specific app-layer patterns from late packets
- Fix: 10-epoch SSL (cosine LR 5e-5‚Üí0) + Œª=1.0 (10√ó stronger contrastive preservation)

#### New Script: `run_self_distill_v2.py` (668 lines)
- Step 1: SSL pretrain 10 epochs (NT-Xent œÑ=0.5 + MSE recon + AntiShortcut masking + CutMix 40%)
- Step 2: k-NN eval BEFORE any self-distill (user's key requirement ‚Äî baseline read)
- Step 3: Self-distill 3 epochs LR=1e-5, Œª=1.0 √ó NT-Xent(rep@8) + MSE(rep@8‚Üísg(rep@32))
- Step 4: Post-distill full eval

#### SSL Training (Step 1) ‚Äî 10 Epochs
| Epoch | Loss  |
|-------|-------|
| 1     | 17.0939 |
| 2     | 14.8225 |
| 3     | 14.5920 |
| 4     | 14.4917 |
| 5     | 14.4078 |
| 6     | 14.3611 |
| 7     | 14.3093 |
| 8     | 14.2847 |
| 9     | 14.2733 |
| 10    | 14.2510 |

#### Pure SSL k-NN Results (Step 2 ‚Äî BEFORE self-distill)
| Dataset | Exit @8  | Exit @32 |
|---------|----------|----------|
| UNSW    | 0.9805   | ‚Äî        |
| CIC     | **0.8540** ‚úÖ | ‚Äî    |

- CIC@8 = 0.8540 **exceeds the 0.80 target** ‚Äî pure SSL before any labeled training
- Compare v1: CIC@8 was 0.5927 after self-distill. v2 pure SSL already beats that.

#### Post-Distill Results (Step 3 ‚Äî AFTER self-distill)
| Dataset | Exit @8  | Change vs pure SSL |
|---------|----------|--------------------|
| UNSW    | 0.9700   | ‚àí0.0105 |
| CIC     | 0.5881   | **‚àí0.2659** ‚ùå |

- Self-distill STILL hurts CIC even with 10-epoch SSL and Œª=1.0
- Fundamental tension confirmed: self-distill and cross-dataset generalization are in conflict for this architecture

#### KEY FINDING: Causal Generalization Gradient (@8 vs @32)
Full @8 vs @32 metrics comparison run via `full_exit_comparison.py`:

| Dataset | Exit | AUC    | F1     | Acc    |
|---------|------|--------|--------|--------|
| UNSW    | @8   | 0.9801 | 0.7362 | 0.9610 |
| UNSW    | @32  | 0.9628 | 0.6023 | 0.9284 |
| CIC     | @8   | **0.8517** | 0.7310 | 0.8932 |
| CIC     | @32  | 0.6137 | 0.4406 | 0.5787 |
| CTU     | @8   | 0.6285 | 0.7333 | 0.6780 |
| CTU     | @32  | 0.5676 | 0.7539 | 0.6696 |

- CIC Œî = **+0.238 AUC** at @8 vs @32 ‚Äî headline cross-dataset result
- Mechanistic explanation: CutMix (40%) swaps late packets during SSL ‚Üí h[8:32] encodes UNSW-specific app-layer patterns; h[0:8] encodes universal TCP handshake / early-flow signals
- `mean(h[0:32])` dilutes universal early signal with domain-specific late signal ‚Üí CIC AUC collapses
- BiMamba/BERT CANNOT show this pattern (bidirectional) ‚Üí **unique thesis contribution**

#### Plots Saved
- `results/exit_comparison_roc.png` ‚Äî ROC curves @8 vs @32 for all 3 datasets
- `results/exit_comparison_bar.png` ‚Äî Bar chart AUC comparison

#### Weight Files
| File | Status | Notes |
|------|--------|-------|
| `weights/self_distill/unimamba_ssl_v2.pth` | ‚úÖ | 10-epoch SSL, 7.6 MB |
| `weights/self_distill/unimamba_selfdist_v2.pth` | ‚úÖ | post-distill, 7.6 MB |

#### Open Issues for Next Session
1. **Self-distill still hurts CIC** ‚Äî consider dropping self-distill entirely and using pure SSL @8 for TED
2. **Next token prediction SSL** ‚Äî user asked about autoregressive pre-training (predict x[t+1] from h[t]); best approach is combined: NT-Xent + next-packet MSE prediction
3. Update TED pipeline to use pure SSL v2 weights (`unimamba_ssl_v2.pth`) as base
4. v4 UniMamba KD training was in-progress (ep 6/10 per Session 3) ‚Äî confirm if still needed given pure SSL @8 already hits 0.85 CIC

#### Key Numbers to Carry Forward
- Pure SSL v2 @8 CIC AUC: **0.8540** (best result, no labels needed)
- Post-distill @8 CIC AUC: **0.5881** (self-distill hurts, avoid it)
- @8 vs @32 CIC gap: **Œî+0.238 AUC** (thesis headline)
- UNSW @8: **0.9801** (strong in-domain)

---

### Session 6 (Feb 23 2026) ‚Äî Multi-model @8 vs @32 comparison + v3 late-mask experiment

#### Part 1: BERT vs BiMamba vs UniMamba @8 and @32 (read-only, no retraining)

**Key finding confirmed:** BERT and BiMamba are bidirectional ‚Äî @8 ‚âà @32 (¬±0.02). Only UniMamba shows a real exit gap because it is causal.

| Model | CIC @8 | CIC @32 | Gap |
|-------|--------|---------|-----|
| BERT SSL (1ep, bidirectional) | 0.6433 | 0.6253 | +0.018 |
| BiMamba SSL (1ep, bidirectional) | 0.5323 | 0.6313 | ‚àí0.099 |
| UniMamba SSL v2 (10ep, causal) | **0.8446** | 0.6125 | **+0.232** |

**Conclusion:** The @8 benefit is unique to UniMamba's causality. BERT/BiMamba cannot demonstrate early-exit benefits because their hidden states already contain all 32 packets' information regardless of exit point.

#### Part 2: UniMamba SSL v3 ‚Äî late-packet masking (70% of positions 8-31 zeroed)

Goal: force @32 to generalise like @8 by preventing model from relying on late packets.

| Model | CIC @8 | CIC @32 |
|-------|--------|---------|
| v2 (random aug) | 0.8446 | 0.6125 |
| v3 (70% late-mask) | 0.7769 | **0.7178** |

Result: @32 improved (+0.106) but @8 degraded (‚àí0.068). Late-mask too aggressive ‚Äî it also corrupts early-packet representations since the model's hidden state for h[8] must summarise a mostly-zeroed sequence.

Saved: `weights/self_distill/unimamba_ssl_v3_late_mask.pth`

#### User decision: simplify SSL design for v4

User wants:
- **One augmentation only** (CutMix, both views same aug ‚Äî not two different aug strategies)
- **NT-Xent only** (drop MSE reconstruction loss ‚Äî simpler, cleaner)
- **Add next-packet prediction loss**: `MSE(Linear(h[t]), x[t+1])` ‚Äî autoregressive loss natural for causal model

Rationale: MSE reconstruction loss adds noise (reconstructing masked features that may not be informative) and two different augmentation views introduce asymmetry. Next-packet prediction is the natural SSL objective for a causal sequential model.

#### Open for next session
- Train v4: CutMix-only aug + NT-Xent + next-packet MSE prediction loss
- Compare v4 @8 and @32 against v2 baseline

---
*Append below this line for new sessions*
