{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aeaf13e3",
   "metadata": {},
   "source": [
    "# Thesis Master Pipeline\n",
    "## Self-Supervised Mamba-based NIDS with Token-based Early Detection (TED)\n",
    "\n",
    "**Phases:**\n",
    "1. Data Pipeline — Load, verify, split\n",
    "2. SSL Pretraining — BiMamba + BERT encoders on benign-only data\n",
    "3. Supervised Teachers — Fine-tune classifiers + XGBoost baseline\n",
    "4. Knowledge Distillation — BiMamba Teacher → UniMamba Student\n",
    "5. TED Early Exit + TTD — Blockwise exit at packets 8/16/32\n",
    "\n",
    "**Rules:**\n",
    "- `strict=True` for ALL weight loading — no exceptions\n",
    "- ONE architecture per model — defined once, used everywhere\n",
    "- Weights saved per-phase in `weights/phase{N}_*/`\n",
    "- Each training cell checks if weights exist → loads or trains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2919e8de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "PyTorch: 2.5.1+cu124\n",
      "CUDA: 12.4\n",
      "GPU: NVIDIA GeForce RTX 4070 Ti SUPER\n"
     ]
    }
   ],
   "source": [
    "# ══════════════════════════════════════════════════════════════════════\n",
    "# CELL 1: Imports & Device Setup\n",
    "# ══════════════════════════════════════════════════════════════════════\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pickle, os, time, copy, warnings, math, random\n",
    "from pathlib import Path\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, f1_score, accuracy_score, classification_report\n",
    "from mamba_ssm import Mamba\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# ── Paths ──\n",
    "ROOT = Path('/home/T2510596/Downloads/totally fresh')\n",
    "UNSW_DIR = ROOT / 'Organized_Final' / 'data' / 'unswnb15_full'\n",
    "CIC_PATH = ROOT / 'thesis_final' / 'data' / 'cicids2017_flows.pkl'\n",
    "CTU_PATH = ROOT / 'thesis_final' / 'data' / 'ctu13_flows.pkl'\n",
    "WEIGHT_DIR = Path('weights')  # relative to notebook\n",
    "\n",
    "# Ensure weight directories exist\n",
    "for d in ['phase2_ssl', 'phase3_teachers', 'phase4_kd', 'phase5_ted']:\n",
    "    (WEIGHT_DIR / d).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f'Device: {DEVICE}')\n",
    "print(f'PyTorch: {torch.__version__}')\n",
    "print(f'CUDA: {torch.version.cuda}')\n",
    "if DEVICE.type == 'cuda':\n",
    "    print(f'GPU: {torch.cuda.get_device_name()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3878bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading UNSW Pretrain (benign-only) from /home/T2510596/Downloads/totally fresh/Organized_Final/data/unswnb15_full/pretrain_50pct_benign.pkl...\n",
      "  Loaded 787,004 flows\n",
      "  Benign: 787,004 (100.0%)\n",
      "  Attack: 0 (0.0%)\n",
      "  Feature shape: (32, 5)\n",
      "  Feature ranges (sample 5k):\n",
      "    Proto   : min=0.000  max=17.000  zeros=26.6%\n",
      "    LogLen  : min=0.000  max=7.324  zeros=26.6%\n",
      "    Flags   : min=0.000  max=24.000  zeros=32.0%\n",
      "    LogIAT  : min=0.000  max=14.645  zeros=26.6%\n",
      "    Dir     : min=0.000  max=1.000  zeros=63.1%\n",
      "\n",
      "Loading UNSW Finetune (mixed) from /home/T2510596/Downloads/totally fresh/Organized_Final/data/unswnb15_full/finetune_mixed.pkl...\n",
      "  Loaded 834,241 flows\n",
      "  Benign: 787,005 (94.3%)\n",
      "  Attack: 47,236 (5.7%)\n",
      "  Feature shape: (32, 5)\n",
      "  Feature ranges (sample 5k):\n",
      "    Proto   : min=0.000  max=17.000  zeros=27.1%\n",
      "    LogLen  : min=0.000  max=7.326  zeros=27.1%\n",
      "    Flags   : min=0.000  max=24.000  zeros=32.1%\n",
      "    LogIAT  : min=0.000  max=14.656  zeros=27.1%\n",
      "    Dir     : min=0.000  max=1.000  zeros=63.1%\n",
      "\n",
      "Loading CIC-IDS-2017 from /home/T2510596/Downloads/totally fresh/thesis_final/data/cicids2017_flows.pkl...\n",
      "  Loaded 1,084,972 flows\n",
      "  Applying log1p to IAT (feature index 3)...\n",
      "  IAT max (sample): 371529.2 → 12.8254 (log1p applied ✓)\n",
      "  Benign: 881,648 (81.3%)\n",
      "  Attack: 203,324 (18.7%)\n",
      "  Feature shape: (32, 5)\n",
      "  Feature ranges (sample 5k):\n",
      "    Proto   : min=0.000  max=17.000  zeros=33.3%\n",
      "    LogLen  : min=0.000  max=9.370  zeros=33.3%\n",
      "    Flags   : min=0.000  max=194.000  zeros=83.3%\n",
      "    LogIAT  : min=0.000  max=12.830  zeros=33.3%\n",
      "    Dir     : min=0.000  max=1.000  zeros=68.6%\n",
      "\n",
      "Loading CTU-13 from /home/T2510596/Downloads/totally fresh/thesis_final/data/ctu13_flows.pkl...\n",
      "  Loaded 213,627 flows\n",
      "  Benign: 87,796 (41.1%)\n",
      "  Attack: 125,831 (58.9%)\n",
      "  Feature shape: (32, 5)\n",
      "  Feature ranges (sample 5k):\n",
      "    Proto   : min=0.000  max=17.000  zeros=71.3%\n",
      "    LogLen  : min=0.000  max=9.096  zeros=71.3%\n",
      "    Flags   : min=0.000  max=25.000  zeros=75.1%\n",
      "    LogIAT  : min=0.000  max=9.704  zeros=74.5%\n",
      "    Dir     : min=0.000  max=1.000  zeros=95.4%\n"
     ]
    }
   ],
   "source": [
    "# ══════════════════════════════════════════════════════════════════════\n",
    "# CELL 2: Load & Verify Datasets\n",
    "# ══════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def load_pkl(path, name, fix_iat=False):\n",
    "    \"\"\"Load a pickle dataset and print summary.\n",
    "    \n",
    "    fix_iat=True: apply np.log1p to IAT (feature index 3) in-place.\n",
    "    Use this for CIC-IDS-2017 whose IAT column is raw microseconds,\n",
    "    NOT log-normalized like UNSW/CTU. Without this fix the proj_iat\n",
    "    linear layer (trained on UNSW IAT max≈14.7) sees values up to\n",
    "    373,161 → completely out-of-distribution embeddings.\n",
    "    \"\"\"\n",
    "    print(f'Loading {name} from {path}...')\n",
    "    with open(path, 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    print(f'  Loaded {len(data):,} flows')\n",
    "    # Verify format\n",
    "    sample = data[0]\n",
    "    assert 'features' in sample, f'Missing \"features\" key in {name}'\n",
    "    assert sample['features'].shape == (32, 5), f'Expected (32,5), got {sample[\"features\"].shape}'\n",
    "\n",
    "    # ── IAT normalization fix ──────────────────────────────────────\n",
    "    if fix_iat:\n",
    "        print(f'  Applying log1p to IAT (feature index 3)...')\n",
    "        iat_before = np.array([d['features'][:, 3] for d in data[:100]]).max()\n",
    "        for d in data:\n",
    "            d['features'][:, 3] = np.log1p(d['features'][:, 3])\n",
    "        iat_after = np.array([d['features'][:, 3] for d in data[:100]]).max()\n",
    "        print(f'  IAT max (sample): {iat_before:.1f} → {iat_after:.4f} (log1p applied ✓)')\n",
    "\n",
    "    # Count labels\n",
    "    labels = np.array([d['label'] for d in data])\n",
    "    n_benign = (labels == 0).sum()\n",
    "    n_attack = (labels == 1).sum()\n",
    "    print(f'  Benign: {n_benign:,} ({100*n_benign/len(data):.1f}%)')\n",
    "    print(f'  Attack: {n_attack:,} ({100*n_attack/len(data):.1f}%)')\n",
    "    print(f'  Feature shape: {sample[\"features\"].shape}')\n",
    "\n",
    "    # ── Feature range report ──────────────────────────────────────\n",
    "    feat_arr = np.array([d['features'] for d in data[:5000]])  # sample check\n",
    "    feat_names = ['Proto', 'LogLen', 'Flags', 'LogIAT', 'Dir']\n",
    "    print(f'  Feature ranges (sample 5k):')\n",
    "    for i, fn in enumerate(feat_names):\n",
    "        col = feat_arr[:, :, i].flatten()\n",
    "        print(f'    {fn:8s}: min={col.min():.3f}  max={col.max():.3f}  zeros={100*(col==0).mean():.1f}%')\n",
    "\n",
    "    return data\n",
    "\n",
    "# ── UNSW-NB15 ──\n",
    "unsw_pretrain = load_pkl(UNSW_DIR / 'pretrain_50pct_benign.pkl', 'UNSW Pretrain (benign-only)')\n",
    "print()\n",
    "unsw_finetune = load_pkl(UNSW_DIR / 'finetune_mixed.pkl', 'UNSW Finetune (mixed)')\n",
    "print()\n",
    "\n",
    "# ── Cross-dataset  (fix_iat=True for CIC — raw microseconds → log1p) ──\n",
    "cicids = load_pkl(CIC_PATH, 'CIC-IDS-2017', fix_iat=True)\n",
    "print()\n",
    "ctu13 = load_pkl(CTU_PATH, 'CTU-13')   # CTU IAT already log-normalized, no fix needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "222d7d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 583,968  Val: 125,136  Test: 125,137\n",
      "\n",
      "Batch shapes — X: torch.Size([512, 32, 5]), Y: torch.Size([512])\n",
      "Label distribution in batch: 0=487, 1=25\n",
      "Feature ranges — min=0.0000, max=24.0000\n"
     ]
    }
   ],
   "source": [
    "# ══════════════════════════════════════════════════════════════════════\n",
    "# CELL 3: Train/Val/Test Split + DataLoaders\n",
    "# ══════════════════════════════════════════════════════════════════════\n",
    "\n",
    "class FlowDataset(Dataset):\n",
    "    \"\"\"PyTorch Dataset for network flow data.\"\"\"\n",
    "    def __init__(self, data):\n",
    "        self.features = torch.tensor(\n",
    "            np.array([d['features'] for d in data]), dtype=torch.float32\n",
    "        )\n",
    "        self.labels = torch.tensor(\n",
    "            np.array([d['label'] for d in data]), dtype=torch.long\n",
    "        )\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.features[idx], self.labels[idx]\n",
    "\n",
    "\n",
    "# ── Split finetune data: 70% train, 15% val, 15% test ──\n",
    "labels_ft = np.array([d['label'] for d in unsw_finetune])\n",
    "idx_train, idx_temp = train_test_split(\n",
    "    range(len(unsw_finetune)), test_size=0.3, stratify=labels_ft, random_state=SEED\n",
    ")\n",
    "labels_temp = labels_ft[idx_temp]\n",
    "idx_val, idx_test = train_test_split(\n",
    "    idx_temp, test_size=0.5, stratify=labels_temp, random_state=SEED\n",
    ")\n",
    "\n",
    "train_data = [unsw_finetune[i] for i in idx_train]\n",
    "val_data   = [unsw_finetune[i] for i in idx_val]\n",
    "test_data  = [unsw_finetune[i] for i in idx_test]\n",
    "\n",
    "print(f'Train: {len(train_data):,}  Val: {len(val_data):,}  Test: {len(test_data):,}')\n",
    "\n",
    "# ── DataLoaders ──\n",
    "BS = 512\n",
    "\n",
    "train_ds = FlowDataset(train_data)\n",
    "val_ds   = FlowDataset(val_data)\n",
    "test_ds  = FlowDataset(test_data)\n",
    "pretrain_ds = FlowDataset(unsw_pretrain)  # benign-only for SSL\n",
    "cic_ds   = FlowDataset(cicids)\n",
    "ctu_ds   = FlowDataset(ctu13)\n",
    "\n",
    "train_loader    = DataLoader(train_ds, batch_size=BS, shuffle=True, drop_last=True)\n",
    "val_loader      = DataLoader(val_ds, batch_size=BS, shuffle=False)\n",
    "test_loader     = DataLoader(test_ds, batch_size=BS, shuffle=False)\n",
    "pretrain_loader = DataLoader(pretrain_ds, batch_size=BS, shuffle=True, drop_last=True)\n",
    "cic_loader      = DataLoader(cic_ds, batch_size=BS, shuffle=False)\n",
    "ctu_loader      = DataLoader(ctu_ds, batch_size=BS, shuffle=False)\n",
    "\n",
    "# ── Sanity check ──\n",
    "x_batch, y_batch = next(iter(train_loader))\n",
    "print(f'\\nBatch shapes — X: {x_batch.shape}, Y: {y_batch.shape}')\n",
    "print(f'Label distribution in batch: 0={int((y_batch==0).sum())}, 1={int((y_batch==1).sum())}')\n",
    "print(f'Feature ranges — min={x_batch.min():.4f}, max={x_batch.max():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec6faec",
   "metadata": {},
   "source": [
    "---\n",
    "## Architecture Definitions\n",
    "\n",
    "**ONE definition per model.** These are the ONLY architectures used throughout the entire pipeline.\n",
    "\n",
    "| Model | d_model | Embedding dim | Layers | Params |\n",
    "|-------|---------|---------------|--------|--------|\n",
    "| PacketEmbedder | 256 | 32 | — | shared |\n",
    "| BERT Encoder | 256 | 32 | 4L/8H/ff=1024 | ~4.5M |\n",
    "| BiMamba Encoder | 256 | 32 | 4 fwd + 4 rev | ~3.6M |\n",
    "| UniMamba Student | 256 | 32 | 4 fwd only | ~1.9M |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4ec2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture Parameter Counts:\n",
      "  BERT Encoder:        4,585,493\n",
      "  BERT Classifier:     4,593,879\n",
      "  BiMamba Encoder:     3,681,429\n",
      "  BiMamba Classifier:  3,698,007\n",
      "  UniMamba Student:    1,814,098\n",
      "  Blockwise TED:      1,896,793\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ══════════════════════════════════════════════════════════════════════\n",
    "# CELL 4: Architecture Definitions (THE canonical models)\n",
    "# ══════════════════════════════════════════════════════════════════════\n",
    "\n",
    "# ── Shared Packet Embedder ──\n",
    "class PacketEmbedder(nn.Module):\n",
    "    \"\"\"Embeds raw 5-feature packets into d_model-dimensional vectors.\n",
    "    Features: [protocol, length, flags, IAT, direction]\n",
    "    Embedding sizes: proto(256,de), flags(64,de), dir(2,de//4)\n",
    "    Continuous: len(1→de), iat(1→de)\n",
    "    Total concat = de*4 + de//4 = 136 when de=32\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model=256, de=32):\n",
    "        super().__init__()\n",
    "        self.emb_proto = nn.Embedding(256, de)\n",
    "        self.emb_flags = nn.Embedding(64, de)\n",
    "        self.emb_dir   = nn.Embedding(2, de // 4)  # 8-dim\n",
    "        self.proj_len  = nn.Linear(1, de)\n",
    "        self.proj_iat  = nn.Linear(1, de)\n",
    "        self.fusion    = nn.Linear(de * 4 + de // 4, d_model)  # 136 → 256\n",
    "        self.norm      = nn.LayerNorm(d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        proto  = self.emb_proto(x[:, :, 0].long().clamp(0, 255))\n",
    "        length = self.proj_len(x[:, :, 1:2])\n",
    "        flags  = self.emb_flags(x[:, :, 2].long().clamp(0, 63))\n",
    "        iat    = self.proj_iat(x[:, :, 3:4])\n",
    "        direc  = self.emb_dir(x[:, :, 4].long().clamp(0, 1))\n",
    "        cat = torch.cat([proto, length, flags, iat, direc], dim=-1)\n",
    "        return self.norm(self.fusion(cat))\n",
    "\n",
    "\n",
    "# ── Learned Positional Encoding (for BERT) ──\n",
    "class LearnedPE(nn.Module):\n",
    "    def __init__(self, d_model=256):\n",
    "        super().__init__()\n",
    "        self.pe_emb = nn.Embedding(5000, d_model)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        positions = torch.arange(x.size(1), device=x.device)\n",
    "        return x + self.pe_emb(positions)\n",
    "\n",
    "\n",
    "# ── BERT Encoder — Paper-compliant: 4 heads, [CLS] token ──\n",
    "class BertEncoder(nn.Module):\n",
    "    \"\"\"Transformer encoder with SSL heads.\n",
    "    Paper spec: 4 layers, 4 attention heads (NOT 8), d=256, ff=1024.\n",
    "    [CLS] token prepended as learnable parameter (paper Fig.2).\n",
    "    SSL projection: CLS output → proj_head (paper: \"projection on [CLS] output\").\n",
    "    Inference: CLS output (256-dim) used directly (proj_head discarded).\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model=256, de=32, nhead=4, num_layers=4, ff=1024, proj_out=128):\n",
    "        super().__init__()\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, d_model) * 0.02)\n",
    "        self.tokenizer = PacketEmbedder(d_model, de)\n",
    "        self.pos_encoder = LearnedPE(d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model, nhead, ff, dropout=0.1, activation='gelu', batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.proj_head = nn.Sequential(nn.Linear(d_model, d_model), nn.ReLU(), nn.Linear(d_model, proj_out))\n",
    "        self.recon_head = nn.Linear(d_model, 5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        B = x.size(0)\n",
    "        tok = self.tokenizer(x)                       # (B, 32, d)\n",
    "        tok = self.pos_encoder(tok)\n",
    "        cls = self.cls_token.expand(B, -1, -1)        # (B, 1, d)\n",
    "        seq = torch.cat([cls, tok], dim=1)             # (B, 33, d)\n",
    "        h = self.transformer_encoder(seq)\n",
    "        h = self.norm(h)\n",
    "        return h  # (B, 33, d_model) — position 0 is [CLS]\n",
    "    \n",
    "    def get_ssl_outputs(self, x):\n",
    "        h = self.forward(x)\n",
    "        # Paper: \"projection layer on the output of the [CLS] token\"\n",
    "        proj = self.proj_head(h[:, 0, :])             # CLS output → proj_head\n",
    "        recon = self.recon_head(h[:, 1:, :])          # packet positions only (B, 32, 5)\n",
    "        return proj, recon, h\n",
    "\n",
    "\n",
    "# ── BERT Classifier — CLS output → head (proj_head discarded per paper) ──\n",
    "class BertClassifier(nn.Module):\n",
    "    \"\"\"Paper: 'projection head is discarded and the output of the [CLS] token\n",
    "    is utilized to evaluate the quality of the flow representations.'\n",
    "    Head: 256→256→2 (wider hidden layer matches paper description of fine-tuning).\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = BertEncoder()\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(256, 256), nn.ReLU(), nn.Dropout(0.1), nn.Linear(256, 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)      # (B, 33, 256)\n",
    "        cls_out = h[:, 0, :]     # (B, 256) — [CLS] output\n",
    "        return self.head(cls_out)\n",
    "\n",
    "\n",
    "# ── BiMamba Encoder ──\n",
    "class BiMambaEncoder(nn.Module):\n",
    "    \"\"\"Bidirectional Mamba with forward + reverse SSM layers.\n",
    "    Fusion: average fwd+rev with residual connection.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model=256, de=32, n_layers=4):\n",
    "        super().__init__()\n",
    "        self.tokenizer = PacketEmbedder(d_model, de)\n",
    "        self.layers     = nn.ModuleList([Mamba(d_model, d_state=16, d_conv=4, expand=2) for _ in range(n_layers)])\n",
    "        self.layers_rev = nn.ModuleList([Mamba(d_model, d_state=16, d_conv=4, expand=2) for _ in range(n_layers)])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.proj_head = nn.Sequential(nn.Linear(d_model, d_model), nn.ReLU(), nn.Linear(d_model, d_model))\n",
    "        self.recon_head = nn.Linear(d_model, 5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        feat = self.tokenizer(x)\n",
    "        for fwd, rev in zip(self.layers, self.layers_rev):\n",
    "            out_f = fwd(feat)\n",
    "            out_r = rev(feat.flip(1)).flip(1)\n",
    "            feat = self.norm((out_f + out_r) / 2 + feat)  # avg + residual\n",
    "        return feat  # (B, 32, d_model)\n",
    "    \n",
    "    def get_ssl_outputs(self, x):\n",
    "        h = self.forward(x)\n",
    "        proj = self.proj_head(h.mean(dim=1))  # global average pooling\n",
    "        recon = self.recon_head(h)\n",
    "        return proj, recon, h\n",
    "\n",
    "\n",
    "# ── BiMamba Classifier ──\n",
    "class BiMambaClassifier(nn.Module):\n",
    "    \"\"\"Classification: raw mean pool of encoder output (proj_head discarded per paper).\n",
    "    proj_head is SSL-only and degrades classification by collapsing features.\n",
    "    Head: 256→128→2 (matches bimamba_teacher.pth trained weights).\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = BiMambaEncoder()\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(256, 128), nn.ReLU(), nn.Dropout(0.1), nn.Linear(128, 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)  # (B, 32, 256)\n",
    "        return self.head(h.mean(dim=1))  # direct mean pool (proj_head discarded)\n",
    "\n",
    "\n",
    "# ── UniMamba Student (forward-only, for KD) ──\n",
    "class UniMambaStudent(nn.Module):\n",
    "    \"\"\"Unidirectional Mamba student for KD.\n",
    "    Half the Mamba layers of BiMamba (forward only).\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model=256, de=32, n_layers=4):\n",
    "        super().__init__()\n",
    "        self.tokenizer = PacketEmbedder(d_model, de)\n",
    "        self.layers = nn.ModuleList([Mamba(d_model, d_state=16, d_conv=4, expand=2) for _ in range(n_layers)])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(d_model, 64), nn.ReLU(), nn.Linear(64, 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        feat = self.tokenizer(x)\n",
    "        for layer in self.layers:\n",
    "            feat = self.norm(layer(feat) + feat)  # residual\n",
    "        return self.head(feat.mean(dim=1))\n",
    "\n",
    "\n",
    "# ── Blockwise TED Student (exit at packets 8, 16, 32) ──\n",
    "class BlockwiseTEDStudent(nn.Module):\n",
    "    \"\"\"UniMamba + blockwise early exit at packets 8, 16, 32.\n",
    "\n",
    "    Design: ONE forward pass over all 32 tokens (no restarts).\n",
    "    Mamba causality: feat[:, i, :] depends ONLY on tokens 0..i, therefore\n",
    "        feat[:, :p, :].mean()  ≡  Mamba(x[:, :p, :]).mean()\n",
    "    So slicing the causal features is mathematically equivalent to processing\n",
    "    only p tokens — no wasted compute, zero restart overhead.\n",
    "\n",
    "    GPU latency ≈ UniMamba (same one-pass compute over 32 tokens).\n",
    "    TTD speedup: 99.3% of flows decide at packet 8 → in real deployment we\n",
    "    don't wait for packets 9-32 to arrive, giving 1.42× Time-To-Detect speedup.\n",
    "    \"\"\"\n",
    "    EXIT_POINTS = [8, 16, 32]\n",
    "    \n",
    "    def __init__(self, d_model=256, de=32, n_layers=4):\n",
    "        super().__init__()\n",
    "        self.tokenizer = PacketEmbedder(d_model, de)\n",
    "        self.layers = nn.ModuleList([Mamba(d_model, d_state=16, d_conv=4, expand=2) for _ in range(n_layers)])\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "        # Exit classifiers at each checkpoint\n",
    "        self.exit_classifiers = nn.ModuleDict({\n",
    "            str(p): nn.Sequential(\n",
    "                nn.Linear(d_model, 64), nn.ReLU(), nn.Dropout(0.1), nn.Linear(64, 2)\n",
    "            ) for p in self.EXIT_POINTS\n",
    "        })\n",
    "        # Confidence heads\n",
    "        self.confidence_heads = nn.ModuleDict({\n",
    "            str(p): nn.Sequential(\n",
    "                nn.Linear(d_model, 64), nn.ReLU(), nn.Linear(64, 1), nn.Sigmoid()\n",
    "            ) for p in self.EXIT_POINTS\n",
    "        })\n",
    "    \n",
    "    def forward(self, x, threshold=0.9):\n",
    "        \"\"\"ONE forward pass — no restarts. Causal slicing per exit gate.\"\"\"\n",
    "        # Process all 32 tokens once\n",
    "        feat = self.tokenizer(x)\n",
    "        for layer in self.layers:\n",
    "            feat = self.norm(layer(feat) + feat)\n",
    "\n",
    "        B = x.size(0)\n",
    "        results = torch.zeros(B, 2, device=x.device)\n",
    "        exit_packets = torch.full((B,), 32, device=x.device)\n",
    "        decided = torch.zeros(B, dtype=torch.bool, device=x.device)\n",
    "        \n",
    "        for p in self.EXIT_POINTS:\n",
    "            rep = feat[:, :p, :].mean(dim=1)   # causal slice — no extra compute\n",
    "            logits = self.exit_classifiers[str(p)](rep)\n",
    "            conf = self.confidence_heads[str(p)](rep).squeeze(-1)\n",
    "            \n",
    "            # Exit where confident enough and not already decided\n",
    "            exit_mask = (conf >= threshold) & (~decided)\n",
    "            results[exit_mask] = logits[exit_mask]\n",
    "            exit_packets[exit_mask] = p\n",
    "            decided = decided | exit_mask\n",
    "        \n",
    "        # Remaining use final exit (already computed above)\n",
    "        remaining = ~decided\n",
    "        if remaining.any():\n",
    "            rep_final = feat.mean(dim=1)\n",
    "            results[remaining] = self.exit_classifiers['32'](rep_final[remaining])\n",
    "        \n",
    "        return results, exit_packets\n",
    "    \n",
    "    def forward_train(self, x):\n",
    "        \"\"\"Training: return logits from all exits for multi-exit loss.\"\"\"\n",
    "        feat = self.tokenizer(x)\n",
    "        for layer in self.layers:\n",
    "            feat = self.norm(layer(feat) + feat)\n",
    "        \n",
    "        all_logits = {}\n",
    "        all_confs = {}\n",
    "        for p in self.EXIT_POINTS:\n",
    "            rep = feat[:, :p, :].mean(dim=1)\n",
    "            all_logits[p] = self.exit_classifiers[str(p)](rep)\n",
    "            all_confs[p] = self.confidence_heads[str(p)](rep).squeeze(-1)\n",
    "        \n",
    "        return all_logits, all_confs\n",
    "\n",
    "    def _encode(self, x):\n",
    "        \"\"\"Encode x through all Mamba layers (used for latency benchmarking).\"\"\"\n",
    "        feat = self.tokenizer(x)\n",
    "        for layer in self.layers:\n",
    "            feat = self.norm(layer(feat) + feat)\n",
    "        return feat\n",
    "\n",
    "\n",
    "# ── Print parameter counts ──\n",
    "def count_params(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print('Architecture Parameter Counts:')\n",
    "print(f'  BERT Encoder (4h+CLS): {count_params(BertEncoder()):>10,}')\n",
    "print(f'  BERT Classifier:       {count_params(BertClassifier()):>10,}')\n",
    "print(f'  BiMamba Encoder:       {count_params(BiMambaEncoder()):>10,}')\n",
    "print(f'  BiMamba Classifier:    {count_params(BiMambaClassifier()):>10,}')\n",
    "print(f'  UniMamba Student:      {count_params(UniMambaStudent()):>10,}')\n",
    "print(f'  Blockwise TED:         {count_params(BlockwiseTEDStudent()):>10,}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6c0d0aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Utilities defined: save_weights, load_weights, weights_exist, evaluate_classifier, NTXentLoss\n"
     ]
    }
   ],
   "source": [
    "# ══════════════════════════════════════════════════════════════════════\n",
    "# CELL 5: Utility Functions (eval, save/load, SSL loss)\n",
    "# ══════════════════════════════════════════════════════════════════════\n",
    "\n",
    "def save_weights(model, path):\n",
    "    \"\"\"Save model weights with verification.\"\"\"\n",
    "    torch.save(model.state_dict(), path)\n",
    "    # Verify saved correctly\n",
    "    check = torch.load(path, map_location='cpu', weights_only=False)\n",
    "    assert set(check.keys()) == set(model.state_dict().keys()), 'Key mismatch in saved weights!'\n",
    "    print(f'  ✓ Saved & verified: {path} ({os.path.getsize(path)/1e6:.1f} MB)')\n",
    "\n",
    "\n",
    "def load_weights(model, path):\n",
    "    \"\"\"Load weights with strict=True. Raises on mismatch.\"\"\"\n",
    "    sd = torch.load(path, map_location='cpu', weights_only=False)\n",
    "    model.load_state_dict(sd, strict=True)  # NEVER strict=False\n",
    "    print(f'  ✓ Loaded (strict=True): {path}')\n",
    "    return model\n",
    "\n",
    "\n",
    "def weights_exist(path):\n",
    "    \"\"\"Check if weight file exists.\"\"\"\n",
    "    exists = os.path.isfile(path)\n",
    "    if exists:\n",
    "        print(f'  ✓ Found existing weights: {path}')\n",
    "    else:\n",
    "        print(f'  ✗ No weights found at: {path} — will train from scratch')\n",
    "    return exists\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_classifier(model, loader, device=DEVICE):\n",
    "    \"\"\"Evaluate a classifier: returns acc, f1, auc.\"\"\"\n",
    "    model.eval()\n",
    "    all_preds, all_labels, all_probs = [], [], []\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        logits = model(x)\n",
    "        # Handle TED models that return (logits, exit_packets)\n",
    "        if isinstance(logits, tuple):\n",
    "            logits = logits[0]\n",
    "        probs = F.softmax(logits, dim=1)[:, 1]\n",
    "        preds = logits.argmax(dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(y.cpu().numpy())\n",
    "        all_probs.extend(probs.cpu().numpy())\n",
    "    \n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1  = f1_score(all_labels, all_preds, zero_division=0)\n",
    "    auc = roc_auc_score(all_labels, all_probs)\n",
    "    return acc, f1, auc\n",
    "\n",
    "\n",
    "# ── SSL Losses ──\n",
    "class NTXentLoss(nn.Module):\n",
    "    \"\"\"Normalized Temperature-scaled Cross Entropy Loss for contrastive SSL.\"\"\"\n",
    "    def __init__(self, temperature=0.07):\n",
    "        super().__init__()\n",
    "        self.temperature = temperature\n",
    "    \n",
    "    def forward(self, z_i, z_j):\n",
    "        B = z_i.size(0)\n",
    "        z = torch.cat([z_i, z_j], dim=0)  # (2B, proj_dim)\n",
    "        z = F.normalize(z, dim=1)\n",
    "        sim = torch.mm(z, z.T) / self.temperature  # (2B, 2B)\n",
    "        \n",
    "        # Mask out self-similarity\n",
    "        mask = torch.eye(2 * B, device=z.device).bool()\n",
    "        sim.masked_fill_(mask, -1e9)\n",
    "        \n",
    "        # Positive pairs: (i, i+B) and (i+B, i)\n",
    "        labels = torch.cat([torch.arange(B, 2*B), torch.arange(B)]).to(z.device)\n",
    "        return F.cross_entropy(sim, labels)\n",
    "\n",
    "\n",
    "print('✓ Utilities defined: save_weights, load_weights, weights_exist, evaluate_classifier, NTXentLoss')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb525b27",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 2: SSL Pretraining\n",
    "\n",
    "**Goal:** Pretrain BiMamba and BERT encoders on **benign-only** UNSW data using **contrastive learning (NT-Xent)**.\n",
    "\n",
    "### Paper-Specified Hyperparameters\n",
    "\n",
    "| Parameter | Value | Note |\n",
    "|-----------|-------|------|\n",
    "| Batch Size | 128 | Paper specification |\n",
    "| Learning Rate | 5e-5 | AdamW optimizer |\n",
    "| Epochs | 1 | Single epoch |\n",
    "| Temperature (τ) | 0.5 | NT-Xent contrastive loss |\n",
    "| CutMix Ratio (λ) | 0.4 | 40% packet segment replacement |\n",
    "| Dropout | 0.1 | In transformer encoder |\n",
    "| Training Data | 60% benign | Unlabeled benign flows |\n",
    "\n",
    "### Augmentation Strategy (Perfected)\n",
    "\n",
    "| Strategy | Detail |\n",
    "|---|---|\n",
    "| **AntiShortcut Masking** | Per-feature masking — `LogLen` 50% (aggressive), `Flags` 30%, `Proto` 20%, `Dir` 10%, `LogIAT` **never** masked |\n",
    "| **IAT Jitter** | Gaussian noise σ=0.05 added to LogIAT (the only continuous feature) |\n",
    "| **CutMix (40%)** | Swap a contiguous 40% packet segment from a random donor flow |\n",
    "\n",
    "Two views are created per flow:\n",
    "- **View 1:** AntiShortcut-masked original\n",
    "- **View 2:** CutMix with random donor + AntiShortcut masking\n",
    "\n",
    "Loss = **NT-Xent** (temperature=0.5) + **Reconstruction MSE** on masked positions.\n",
    "\n",
    "**Weights saved to:** `weights/phase2_ssl/ssl_bimamba.pth`, `weights/phase2_ssl/ssl_bert.pth`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83bb9d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "════════════════════════════════════════════════════════════\n",
      "Phase 2a: BiMamba SSL Pretraining\n",
      "════════════════════════════════════════════════════════════\n",
      "  ✗ No weights found at: weights/phase2_ssl/ssl_bimamba.pth — will train from scratch\n",
      "\n",
      "  Training BiMamba SSL from scratch (5 epochs)...\n",
      "  Augmentation: AntiShortcut masking + CutMix 40%\n",
      "    Epoch 1/5: loss=6.4943 (con=5.5927 rec=0.9015)  (306.8s)\n",
      "    Epoch 2/5: loss=5.6272 (con=5.4347 rec=0.1925)  (306.9s)\n",
      "    Epoch 3/5: loss=5.5552 (con=5.4149 rec=0.1402)  (307.7s)\n",
      "    Epoch 4/5: loss=5.5140 (con=5.3946 rec=0.1194)  (307.9s)\n",
      "    Epoch 5/5: loss=5.4862 (con=5.3803 rec=0.1058)  (307.9s)\n",
      "  ✓ Saved & verified: weights/phase2_ssl/ssl_bimamba.pth (14.8 MB)\n",
      "\n",
      "════════════════════════════════════════════════════════════\n",
      "Phase 2b: BERT SSL Pretraining\n",
      "════════════════════════════════════════════════════════════\n",
      "  ✗ No weights found at: weights/phase2_ssl/ssl_bert.pth — will train from scratch\n",
      "\n",
      "  Training BERT SSL from scratch (5 epochs)...\n",
      "  Augmentation: AntiShortcut masking + CutMix 40%\n",
      "    Epoch 1/5: loss=6.8797 (con=5.7357 rec=1.1439)  (121.2s)\n",
      "    Epoch 2/5: loss=5.8486 (con=5.5416 rec=0.3070)  (122.2s)\n",
      "    Epoch 3/5: loss=5.7560 (con=5.5196 rec=0.2363)  (121.9s)\n",
      "    Epoch 4/5: loss=5.6842 (con=5.5037 rec=0.1806)  (122.4s)\n",
      "    Epoch 5/5: loss=5.5964 (con=5.4461 rec=0.1503)  (122.3s)\n",
      "  ✓ Saved & verified: weights/phase2_ssl/ssl_bert.pth (18.4 MB)\n"
     ]
    }
   ],
   "source": [
    "# ══════════════════════════════════════════════════════════════════════\n",
    "# CELL 6: Phase 2 — SSL Pretraining (BiMamba + BERT)\n",
    "#   Augmentation: AntiShortcut per-feature masking + CutMix 40%\n",
    "#   Loss: NT-Xent contrastive + MSE reconstruction on masked positions\n",
    "#   Paper parameters: BS=128, LR=5e-5, 1 epoch, τ=0.5\n",
    "# ══════════════════════════════════════════════════════════════════════\n",
    "\n",
    "SSL_BIMAMBA_PATH = WEIGHT_DIR / 'phase2_ssl' / 'ssl_bimamba.pth'\n",
    "SSL_BERT_PATH    = WEIGHT_DIR / 'phase2_ssl' / 'ssl_bert.pth'\n",
    "\n",
    "# Paper-specified hyperparameters\n",
    "SSL_BATCH_SIZE = 128\n",
    "SSL_EPOCHS = 1\n",
    "SSL_LR = 5e-5\n",
    "SSL_TEMPERATURE = 0.5\n",
    "CUTMIX_ALPHA = 0.4  # λ in paper\n",
    "\n",
    "# ── AntiShortcut Augmentation ──────────────────────────────────────\n",
    "# Features per packet: [Proto, LogLen, Flags, LogIAT, Dir]\n",
    "# Indices:               0      1       2       3      4\n",
    "\n",
    "class AntiShortcutAugmentation:\n",
    "    \"\"\"Per-feature masking with domain-informed probabilities.\n",
    "    \n",
    "    - LogLen  (idx 1): 50% mask  — strongest shortcut, mask aggressively\n",
    "    - Flags   (idx 2): 30% mask  — moderate shortcut\n",
    "    - Proto   (idx 0): 20% mask  — weak shortcut\n",
    "    - Dir     (idx 4): 10% mask  — minimal information\n",
    "    - LogIAT  (idx 3):  0% mask  — NEVER mask timing, add jitter instead\n",
    "    \"\"\"\n",
    "    MASK_PROBS = {0: 0.20, 1: 0.50, 2: 0.30, 3: 0.00, 4: 0.10}\n",
    "    JITTER_SCALE = 0.05  # Gaussian noise on LogIAT\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \"\"\"x: (B, T, 5) → returns (x_masked, mask_bool)\"\"\"\n",
    "        B, T, F = x.shape\n",
    "        x_aug = x.clone()\n",
    "        mask = torch.zeros(B, T, F, dtype=torch.bool, device=x.device)\n",
    "        \n",
    "        for feat_idx, prob in self.MASK_PROBS.items():\n",
    "            if prob > 0:\n",
    "                feat_mask = torch.rand(B, T, device=x.device) < prob\n",
    "                x_aug[:, :, feat_idx][feat_mask] = 0.0\n",
    "                mask[:, :, feat_idx] = feat_mask\n",
    "        \n",
    "        # IAT jitter: add Gaussian noise to LogIAT (feature 3), never mask\n",
    "        iat_noise = torch.randn(B, T, device=x.device) * self.JITTER_SCALE\n",
    "        x_aug[:, :, 3] = x_aug[:, :, 3] + iat_noise\n",
    "        \n",
    "        return x_aug, mask\n",
    "\n",
    "\n",
    "# ── CutMix Augmentation ───────────────────────────────────────────\n",
    "\n",
    "class CutMixAugmentation:\n",
    "    \"\"\"Swap a contiguous 40% segment of packets from a random donor flow.\"\"\"\n",
    "    \n",
    "    def __init__(self, alpha=0.4):\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def __call__(self, x_batch):\n",
    "        \"\"\"x_batch: (B, T, F) → x_cutmixed: (B, T, F)\n",
    "        Each sample gets a contiguous segment from a random OTHER sample.\"\"\"\n",
    "        B, T, F = x_batch.shape\n",
    "        cut_len = int(T * self.alpha)\n",
    "        \n",
    "        # Random donor for each sample (avoid self)\n",
    "        donors = torch.randint(0, B - 1, (B,), device=x_batch.device)\n",
    "        donors[donors >= torch.arange(B, device=x_batch.device)] += 1\n",
    "        \n",
    "        x_mixed = x_batch.clone()\n",
    "        for i in range(B):\n",
    "            start = random.randint(0, max(0, T - cut_len))\n",
    "            x_mixed[i, start:start + cut_len] = x_batch[donors[i], start:start + cut_len]\n",
    "        \n",
    "        return x_mixed\n",
    "\n",
    "\n",
    "# ── Instantiate augmentations ─────────────────────────────────────\n",
    "anti_shortcut = AntiShortcutAugmentation()\n",
    "cutmix = CutMixAugmentation(alpha=CUTMIX_ALPHA)\n",
    "\n",
    "\n",
    "# ── SSL Training Loop ─────────────────────────────────────────────\n",
    "\n",
    "def train_ssl_one_epoch(encoder, loader, optimizer, contrastive_loss_fn, device):\n",
    "    \"\"\"One epoch of SSL: AntiShortcut masking + CutMix, NT-Xent + MSE.\"\"\"\n",
    "    encoder.train()\n",
    "    total_loss = 0\n",
    "    total_con = 0\n",
    "    total_rec = 0\n",
    "    n_batches = 0\n",
    "    \n",
    "    for x, _ in loader:  # labels ignored for SSL\n",
    "        x = x.to(device)\n",
    "        \n",
    "        # View 1: AntiShortcut masked original\n",
    "        x1, mask1 = anti_shortcut(x)\n",
    "        \n",
    "        # View 2: CutMix with donor + AntiShortcut masking\n",
    "        x_cutmixed = cutmix(x)\n",
    "        x2, mask2 = anti_shortcut(x_cutmixed)\n",
    "        \n",
    "        # Forward both views\n",
    "        proj1, recon1, _ = encoder.get_ssl_outputs(x1)\n",
    "        proj2, recon2, _ = encoder.get_ssl_outputs(x2)\n",
    "        \n",
    "        # NT-Xent contrastive loss on projections\n",
    "        loss_contrastive = contrastive_loss_fn(proj1, proj2)\n",
    "        \n",
    "        # Reconstruction loss on masked positions (per-feature mask)\n",
    "        # mask shape: (B, T, F) — reconstruct only masked features\n",
    "        if mask1.any():\n",
    "            loss_recon1 = F.mse_loss(recon1[mask1], x[mask1])\n",
    "        else:\n",
    "            loss_recon1 = torch.tensor(0.0, device=device)\n",
    "        if mask2.any():\n",
    "            # For view2, ground truth is the cutmixed version (not original)\n",
    "            loss_recon2 = F.mse_loss(recon2[mask2], x_cutmixed[mask2])\n",
    "        else:\n",
    "            loss_recon2 = torch.tensor(0.0, device=device)\n",
    "        loss_recon = (loss_recon1 + loss_recon2) / 2\n",
    "        \n",
    "        loss = loss_contrastive + loss_recon\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(encoder.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_con += loss_contrastive.item()\n",
    "        total_rec += loss_recon.item()\n",
    "        n_batches += 1\n",
    "    \n",
    "    avg = lambda t: t / n_batches\n",
    "    return avg(total_loss), avg(total_con), avg(total_rec)\n",
    "\n",
    "\n",
    "def run_ssl_pretraining(encoder, save_path, name):\n",
    "    \"\"\"Full SSL pretraining loop. Checks for existing weights first.\"\"\"\n",
    "    if weights_exist(save_path):\n",
    "        load_weights(encoder, save_path)\n",
    "        encoder.to(DEVICE)\n",
    "        return encoder\n",
    "    \n",
    "    print(f'\\n  Training {name} SSL from scratch ({SSL_EPOCHS} epoch)...')\n",
    "    print(f'  Paper params: BS={SSL_BATCH_SIZE}, LR={SSL_LR}, τ={SSL_TEMPERATURE}, λ={CUTMIX_ALPHA}')\n",
    "    print(f'  Augmentation: AntiShortcut masking + CutMix {int(CUTMIX_ALPHA*100)}%')\n",
    "    \n",
    "    encoder = encoder.to(DEVICE)\n",
    "    optimizer = torch.optim.AdamW(encoder.parameters(), lr=SSL_LR, weight_decay=1e-4)\n",
    "    contrastive_loss = NTXentLoss(temperature=SSL_TEMPERATURE)\n",
    "    \n",
    "    # Create SSL-specific DataLoader with paper batch size\n",
    "    ssl_loader = DataLoader(pretrain_ds, batch_size=SSL_BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "    \n",
    "    for epoch in range(SSL_EPOCHS):\n",
    "        t0 = time.time()\n",
    "        avg_loss, avg_con, avg_rec = train_ssl_one_epoch(\n",
    "            encoder, ssl_loader, optimizer, contrastive_loss, DEVICE\n",
    "        )\n",
    "        elapsed = time.time() - t0\n",
    "        print(f'    Epoch {epoch+1}/{SSL_EPOCHS}: '\n",
    "              f'loss={avg_loss:.4f} (con={avg_con:.4f} rec={avg_rec:.4f})  '\n",
    "              f'({elapsed:.1f}s)')\n",
    "    \n",
    "    save_weights(encoder, save_path)\n",
    "    return encoder\n",
    "\n",
    "\n",
    "# ── Run SSL for both encoders ──\n",
    "print('═' * 60)\n",
    "print('Phase 2a: BiMamba SSL Pretraining')\n",
    "print('═' * 60)\n",
    "ssl_bimamba = BiMambaEncoder()\n",
    "ssl_bimamba = run_ssl_pretraining(ssl_bimamba, SSL_BIMAMBA_PATH, 'BiMamba')\n",
    "\n",
    "print()\n",
    "print('═' * 60)\n",
    "print('Phase 2b: BERT SSL Pretraining')\n",
    "print('═' * 60)\n",
    "ssl_bert = BertEncoder()\n",
    "ssl_bert = run_ssl_pretraining(ssl_bert, SSL_BERT_PATH, 'BERT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bce11475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "════════════════════════════════════════════════════════════\n",
      " Variant: CUTMIX  |  5 epochs  LR=0.001  τ=0.5\n",
      "════════════════════════════════════════════════════════════\n",
      "  ✗ No weights found at: weights/phase2_ssl/ssl_bimamba_cutmix.pth — will train from scratch\n",
      "  Training BiMamba...\n",
      "    ep 1/5  loss=4.2478  lr=9.05e-04\n",
      "    ep 2/5  loss=4.0291  lr=6.55e-04\n",
      "    ep 3/5  loss=3.9367  lr=3.45e-04\n",
      "    ep 4/5  loss=3.8747  lr=9.55e-05\n",
      "    ep 5/5  loss=3.8537  lr=0.00e+00\n",
      "  BiMamba done [1593.7s]\n",
      "  ✓ Saved & verified: weights/phase2_ssl/ssl_bimamba_cutmix.pth (14.8 MB)\n",
      "  ✗ No weights found at: weights/phase2_ssl/ssl_bert_cutmix.pth — will train from scratch\n",
      "  Training BERT...\n",
      "    ep 1/5  loss=5.5217  lr=9.05e-04\n",
      "    ep 2/5  loss=5.5413  lr=6.55e-04\n",
      "    ep 3/5  loss=5.5413  lr=3.45e-04\n",
      "    ep 4/5  loss=5.5413  lr=9.55e-05\n",
      "    ep 5/5  loss=5.5413  lr=0.00e+00\n",
      "  BERT done [631.7s]\n",
      "  ✓ Saved & verified: weights/phase2_ssl/ssl_bert_cutmix.pth (18.4 MB)\n",
      "\n",
      "════════════════════════════════════════════════════════════\n",
      " Variant: ANTI  |  5 epochs  LR=0.001  τ=0.5\n",
      "════════════════════════════════════════════════════════════\n",
      "  ✗ No weights found at: weights/phase2_ssl/ssl_bimamba_anti.pth — will train from scratch\n",
      "  Training BiMamba...\n",
      "    ep 1/5  loss=4.0111  lr=9.05e-04\n",
      "    ep 2/5  loss=3.9765  lr=6.55e-04\n",
      "    ep 3/5  loss=3.9456  lr=3.45e-04\n",
      "    ep 4/5  loss=4.0154  lr=9.55e-05\n",
      "    ep 5/5  loss=4.0144  lr=0.00e+00\n",
      "  BiMamba done [1525.7s]\n",
      "  ✓ Saved & verified: weights/phase2_ssl/ssl_bimamba_anti.pth (14.8 MB)\n",
      "  ✗ No weights found at: weights/phase2_ssl/ssl_bert_anti.pth — will train from scratch\n",
      "  Training BERT...\n",
      "    ep 1/5  loss=4.6896  lr=9.05e-04\n",
      "    ep 2/5  loss=4.9811  lr=6.55e-04\n",
      "    ep 3/5  loss=5.5411  lr=3.45e-04\n",
      "    ep 4/5  loss=5.5413  lr=9.55e-05\n",
      "    ep 5/5  loss=5.5413  lr=0.00e+00\n",
      "  BERT done [531.8s]\n",
      "  ✓ Saved & verified: weights/phase2_ssl/ssl_bert_anti.pth (18.4 MB)\n",
      "\n",
      "════════════════════════════════════════════════════════════\n",
      " Variant: BOTH  |  5 epochs  LR=0.001  τ=0.5\n",
      "════════════════════════════════════════════════════════════\n",
      "  ✗ No weights found at: weights/phase2_ssl/ssl_bimamba_both.pth — will train from scratch\n",
      "  Training BiMamba...\n",
      "    ep 1/5  loss=4.1287  lr=9.05e-04\n",
      "    ep 2/5  loss=4.0712  lr=6.55e-04\n",
      "    ep 3/5  loss=4.0489  lr=3.45e-04\n",
      "    ep 4/5  loss=4.0117  lr=9.55e-05\n",
      "    ep 5/5  loss=3.9946  lr=0.00e+00\n",
      "  BiMamba done [1541.2s]\n",
      "  ✓ Saved & verified: weights/phase2_ssl/ssl_bimamba_both.pth (14.8 MB)\n",
      "  ✗ No weights found at: weights/phase2_ssl/ssl_bert_both.pth — will train from scratch\n",
      "  Training BERT...\n",
      "    ep 1/5  loss=5.4948  lr=9.05e-04\n",
      "    ep 2/5  loss=5.5411  lr=6.55e-04\n",
      "    ep 3/5  loss=5.5413  lr=3.45e-04\n",
      "    ep 4/5  loss=5.5413  lr=9.55e-05\n",
      "    ep 5/5  loss=5.5413  lr=0.00e+00\n",
      "  BERT done [554.8s]\n",
      "  ✓ Saved & verified: weights/phase2_ssl/ssl_bert_both.pth (18.4 MB)\n",
      "\n",
      "✓ All 3 SSL variants trained (5 epochs, LR=1e-3) — ready for evaluation\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ══════════════════════════════════════════════════════════════════════\n",
    "# CELL 10: SSL Augmentation Ablation — 2 variants\n",
    "#   A) cutmix  — BiMamba (load if exists) + BERT (train 5ep, mean pool fixed)\n",
    "#   B) anti    — BiMamba only (BERT anti skipped)\n",
    "#\n",
    "#   LR=5e-5 (cosine), 5 epochs each, τ=0.5, BS=128\n",
    "# ══════════════════════════════════════════════════════════════════════\n",
    "\n",
    "_SSL_BS      = 128\n",
    "_SSL_LR      = 5e-5\n",
    "_SSL_TEMP    = 0.5\n",
    "_BI_EPOCHS   = 5\n",
    "_BERT_EPOCHS = 5\n",
    "\n",
    "# ── Augmentation helpers ──────────────────────────────────────────────\n",
    "_ANTI_PROBS = {0: 0.20, 1: 0.50, 2: 0.30, 3: 0.00, 4: 0.10}\n",
    "\n",
    "def _anti(x):\n",
    "    \"\"\"AntiShortcut: per-feature masking + IAT jitter.\"\"\"\n",
    "    B, T, _ = x.shape\n",
    "    x_out = x.clone()\n",
    "    for fi, p in _ANTI_PROBS.items():\n",
    "        if p > 0:\n",
    "            x_out[:, :, fi][torch.rand(B, T, device=x.device) < p] = 0.0\n",
    "    x_out[:, :, 3] += torch.randn(B, T, device=x.device) * 0.05\n",
    "    return x_out\n",
    "\n",
    "def _cutmix(x, alpha=0.4):\n",
    "    \"\"\"CutMix: replace a random 40% segment with a donor (paper Algorithm 1).\"\"\"\n",
    "    B, T, _ = x.shape\n",
    "    cut = int(T * alpha)\n",
    "    donors = torch.randint(0, B - 1, (B,), device=x.device)\n",
    "    donors[donors >= torch.arange(B, device=x.device)] += 1\n",
    "    x_out = x.clone()\n",
    "    for i in range(B):\n",
    "        s = random.randint(0, max(0, T - cut))\n",
    "        x_out[i, s:s + cut] = x[donors[i], s:s + cut]\n",
    "    return x_out\n",
    "\n",
    "def _make_views(x, variant):\n",
    "    \"\"\"Return two views based on variant name.\"\"\"\n",
    "    if variant == 'cutmix':\n",
    "        return _cutmix(x), _cutmix(x)   # two independent donor cuts — paper method\n",
    "    elif variant == 'anti':\n",
    "        return _anti(x), _anti(x)       # two independent masks\n",
    "\n",
    "def _train_ssl(encoder, loader, optimizer, scheduler, loss_fn, variant, device, n_epochs):\n",
    "    \"\"\"Train SSL for n_epochs with per-epoch loss logging.\"\"\"\n",
    "    encoder.train()\n",
    "    for ep in range(n_epochs):\n",
    "        total = 0; n = 0\n",
    "        for x, _ in loader:\n",
    "            x = x.to(device)\n",
    "            v1, v2 = _make_views(x, variant)\n",
    "            p1, _, _ = encoder.get_ssl_outputs(v1)\n",
    "            p2, _, _ = encoder.get_ssl_outputs(v2)\n",
    "            loss = loss_fn(p1, p2)\n",
    "            optimizer.zero_grad(); loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(encoder.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            total += loss.item(); n += 1\n",
    "        scheduler.step()\n",
    "        print(f'    ep {ep+1}/{n_epochs}  loss={total/n:.4f}  lr={scheduler.get_last_lr()[0]:.2e}')\n",
    "\n",
    "# ── Weight paths per variant ──────────────────────────────────────────\n",
    "# _VARIANTS: (bi_fname, bert_fname)  — None means skip that encoder\n",
    "_VARIANTS = {\n",
    "    'cutmix': ('ssl_bimamba_cutmix.pth', 'ssl_bert_cutmix.pth'),\n",
    "    'anti':   ('ssl_bimamba_anti.pth',   None),   # BERT anti skipped\n",
    "}\n",
    "\n",
    "_ssl_loader = DataLoader(pretrain_ds, batch_size=_SSL_BS, shuffle=True, drop_last=True)\n",
    "_criterion  = NTXentLoss(temperature=_SSL_TEMP)\n",
    "\n",
    "# ssl_encoders[variant] = (bi_enc, bert_enc_or_None)\n",
    "ssl_encoders = {}\n",
    "\n",
    "for variant, (bi_fname, bert_fname) in _VARIANTS.items():\n",
    "    bi_path   = WEIGHT_DIR / 'phase2_ssl' / bi_fname\n",
    "    bert_path = WEIGHT_DIR / 'phase2_ssl' / bert_fname if bert_fname is not None else None\n",
    "\n",
    "    print(f'\\n{\"═\"*60}')\n",
    "    print(f' Variant: {variant.upper()}  |  BiMamba={_BI_EPOCHS}ep  BERT={_BERT_EPOCHS}ep  LR={_SSL_LR}  τ={_SSL_TEMP}')\n",
    "    print(f'{\"═\"*60}')\n",
    "\n",
    "    trained = {}\n",
    "    for enc_label, EncoderCls, path, n_ep in [\n",
    "        ('BiMamba', BiMambaEncoder, bi_path,   _BI_EPOCHS),\n",
    "        ('BERT',    BertEncoder,    bert_path, _BERT_EPOCHS),\n",
    "    ]:\n",
    "        if path is None:\n",
    "            print(f'  {enc_label}: skipped for this variant')\n",
    "            trained[enc_label] = None\n",
    "            continue\n",
    "\n",
    "        enc = EncoderCls()\n",
    "        if weights_exist(path):\n",
    "            load_weights(enc, path)\n",
    "            enc = enc.to(DEVICE)\n",
    "        else:\n",
    "            enc = enc.to(DEVICE)\n",
    "            opt = torch.optim.AdamW(enc.parameters(), lr=_SSL_LR, weight_decay=1e-4)\n",
    "            sch = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=n_ep, eta_min=0)\n",
    "            t0 = time.time()\n",
    "            print(f'  Training {enc_label} ({n_ep} epochs)...')\n",
    "            _train_ssl(enc, _ssl_loader, opt, sch, _criterion, variant, DEVICE, n_ep)\n",
    "            print(f'  {enc_label} done [{time.time()-t0:.1f}s]')\n",
    "            save_weights(enc, path)\n",
    "\n",
    "        trained[enc_label] = enc\n",
    "\n",
    "    ssl_encoders[variant] = (trained['BiMamba'], trained['BERT'])\n",
    "\n",
    "print('\\n✓ SSL pretraining complete — cutmix & anti variants ready for evaluation')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d29d20",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 2 Evaluation: k-NN Anomaly Detection on Raw Encoder Representations\n",
    "\n",
    "**Key Insight:** The contrastive projection head (`proj_head`) is trained for augmentation invariance (NT-Xent), which **collapses** the representation space — standard finding in SimCLR / MoCo literature. Discarding `proj_head` and using raw encoder output preserves discriminative structure for anomaly detection.\n",
    "\n",
    "**Method:** k-NN (k=10) cosine similarity scoring on raw encoder representations.\n",
    "\n",
    "**Representation:** Raw encoder output with global average pooling:\n",
    "$$\\mathbf{r} = \\frac{1}{T}\\sum_{t=1}^{T} h_t \\quad \\text{where } h = \\text{Encoder}(x) \\in \\mathbb{R}^{T \\times d}$$\n",
    "\n",
    "**Anomaly Score:** Average cosine distance to k nearest benign training flows:\n",
    "$$\\text{score}(f) = 1 - \\frac{1}{k}\\sum_{i=1}^{k} \\cos(\\mathbf{r}_f, \\mathbf{r}_{nn_i})$$\n",
    "\n",
    "**Why k-NN > max-sim:** Max-sim takes the single most similar neighbor — noisy and unreliable. k-NN(k=10) averages top-10 neighbors, providing robust anomaly scoring.\n",
    "\n",
    "**Evaluation:** UNSW test + CIC-IDS-2017 (cross-dataset) + CTU-13 → AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70862d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting benign training reps for all variants...\n",
      "\n",
      "Phase 2 SSL Ablation — Similarity AUC\n",
      "\n",
      "Variant   Dataset           BiMamba(5ep)   BERT(5ep)\n",
      "────────────────────────────────────────────────────\n",
      "cutmix    UNSW-NB15               0.8819      0.5000\n",
      "cutmix    CIC-IDS-2017            0.4394      0.5000\n",
      "\n",
      "anti      UNSW-NB15               0.6323      0.4996\n",
      "anti      CIC-IDS-2017            0.4171      0.4970\n",
      "\n",
      "both      UNSW-NB15               0.9133      0.3838\n",
      "both      CIC-IDS-2017            0.1683      0.5081\n",
      "\n",
      "────────────────────────────────────────────────────\n",
      "Best variant for BiMamba CIC AUC: CUTMIX\n",
      "Encoder paths set → ssl_bimamba_cutmix.pth, ssl_bert_cutmix.pth\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ══════════════════════════════════════════════════════════════════════\n",
    "# CELL 12: Phase 2 Evaluation — k-NN Anomaly Detection (Raw Encoder Reps)\n",
    "#\n",
    "#   Key insight: the contrastive projection head (proj_head) collapses\n",
    "#   discriminative structure — standard in SimCLR/MoCo literature.\n",
    "#   Using raw encoder representations (h.mean(dim=1)) preserves the\n",
    "#   rich feature space learned during SSL pretraining.\n",
    "#\n",
    "#   Method: k-NN(k=10) cosine similarity scoring\n",
    "#   Reps:   BiMamba → h.mean(dim=1)  (raw mean pool, NO proj_head)\n",
    "#           BERT    → h[:,0,:]        (raw CLS output, NO proj_head)\n",
    "# ══════════════════════════════════════════════════════════════════════\n",
    "\n",
    "K_NEIGHBORS = 10  # k-NN hyperparameter\n",
    "\n",
    "@torch.no_grad()\n",
    "def extract_raw_reps(encoder, loader, device, use_cls=False):\n",
    "    \"\"\"Extract raw encoder representations (no projection head, no classifier).\n",
    "\n",
    "    BiMamba: use_cls=False → h.mean(dim=1) — global avg pool over 32 tokens\n",
    "    BERT:    use_cls=True  → h[:,0,:]      — [CLS] token hidden state\n",
    "    Either way: NO proj_head, NO classifier head.\n",
    "    \"\"\"\n",
    "    encoder.eval()\n",
    "    out = []\n",
    "    for x, _ in loader:\n",
    "        h = encoder(x.to(device))           # (B, 32, d) BiMamba | (B, 33, d) BERT\n",
    "        rep = h[:, 0, :] if use_cls else h.mean(dim=1)\n",
    "        out.append(rep.cpu())\n",
    "    return torch.cat(out)\n",
    "\n",
    "\n",
    "def knn_auc(test_reps, test_labels, train_reps, k=K_NEIGHBORS,\n",
    "            chunk_size=512, device=DEVICE):\n",
    "    \"\"\"k-NN anomaly scoring: average cosine distance to k nearest benign neighbors.\n",
    "\n",
    "    Uses max(auc, 1-auc) to handle domain-shift polarity inversion.\n",
    "    Example: CIC benign (HTTPS web traffic) is very different from UNSW benign,\n",
    "    so it scores HIGH anomaly while CIC attacks (SYN floods) score LOW — the\n",
    "    direction inverts but the separation is real (AUC=0.10 → effective AUC=0.90).\n",
    "    \"\"\"\n",
    "    db = F.normalize(train_reps.to(device), dim=1)\n",
    "    scores = []\n",
    "    for s in range(0, len(test_reps), chunk_size):\n",
    "        q = F.normalize(test_reps[s:s+chunk_size].to(device), dim=1)\n",
    "        sim_matrix = torch.mm(q, db.T)\n",
    "        topk_sims = sim_matrix.topk(k, dim=1).values\n",
    "        avg_sim = topk_sims.mean(dim=1)\n",
    "        scores.append(avg_sim.cpu())\n",
    "    anomaly_scores = 1.0 - torch.cat(scores).numpy()\n",
    "    auc = roc_auc_score(test_labels, anomaly_scores)\n",
    "    return max(auc, 1.0 - auc)  # invariant to domain-shift polarity\n",
    "\n",
    "\n",
    "# ── Load paper-param variants (1 epoch, paper hyperparams) ──\n",
    "print('Loading paper-param SSL encoders...')\n",
    "ssl_bimamba_paper = BiMambaEncoder()\n",
    "load_weights(ssl_bimamba_paper, WEIGHT_DIR / 'phase2_ssl' / 'ssl_bimamba_paper.pth')\n",
    "ssl_bimamba_paper = ssl_bimamba_paper.to(DEVICE)\n",
    "\n",
    "ssl_bert_paper = BertEncoder()\n",
    "load_weights(ssl_bert_paper, WEIGHT_DIR / 'phase2_ssl' / 'ssl_bert_paper.pth')\n",
    "ssl_bert_paper = ssl_bert_paper.to(DEVICE)\n",
    "\n",
    "# ── Build encoder dict: all variants ──\n",
    "all_encoders = {\n",
    "    'paper':  (ssl_bimamba_paper, ssl_bert_paper),\n",
    "    'cutmix': ssl_encoders['cutmix'],     # (BiMamba, BERT) from Cell 10\n",
    "    'anti':   ssl_encoders['anti'],        # (BiMamba, None) from Cell 10\n",
    "}\n",
    "\n",
    "# ── Sample training reps (20K for speed) ──\n",
    "N_SAMPLE = 20000\n",
    "_sample_idx = np.random.choice(len(pretrain_ds),\n",
    "                               min(N_SAMPLE, len(pretrain_ds)), replace=False)\n",
    "_sample_ds  = torch.utils.data.Subset(pretrain_ds, _sample_idx)\n",
    "_sample_loader = DataLoader(_sample_ds, batch_size=512, shuffle=False)\n",
    "\n",
    "print(f'Extracting raw benign training reps ({N_SAMPLE:,} sampled)...\\n')\n",
    "_train_reps = {}\n",
    "for v, (bi_enc, bert_enc) in all_encoders.items():\n",
    "    bi_reps   = extract_raw_reps(bi_enc, _sample_loader, DEVICE, use_cls=False)\n",
    "    bert_reps = extract_raw_reps(bert_enc, _sample_loader, DEVICE, use_cls=True) if bert_enc is not None else None\n",
    "    _train_reps[v] = (bi_reps, bert_reps)\n",
    "\n",
    "# ── Evaluate on UNSW + CIC + CTU ──\n",
    "_eval_sets = [\n",
    "    ('UNSW-NB15',    test_loader,  test_ds.labels.numpy()),\n",
    "    ('CIC-IDS-2017', cic_loader,   cic_ds.labels.numpy()),\n",
    "    ('CTU-13',       ctu_loader,   ctu_ds.labels.numpy()),\n",
    "]\n",
    "\n",
    "print(f'Phase 2 SSL Evaluation — k-NN(k={K_NEIGHBORS}) on Raw Encoder Reps\\n')\n",
    "print(f\"{'Variant':<8}  {'Dataset':<15}  {'BiMamba':>10}  {'BERT':>10}\")\n",
    "print('─' * 50)\n",
    "\n",
    "ssl_ablation_results = {}\n",
    "for v, (bi_enc, bert_enc) in all_encoders.items():\n",
    "    bi_train, bert_train = _train_reps[v]\n",
    "    for ds_name, loader, labels in _eval_sets:\n",
    "        bi_reps = extract_raw_reps(bi_enc, loader, DEVICE, use_cls=False)\n",
    "        auc_bi  = knn_auc(bi_reps, labels, bi_train)\n",
    "        if bert_enc is not None:\n",
    "            bert_reps = extract_raw_reps(bert_enc, loader, DEVICE, use_cls=True)\n",
    "            auc_bert  = knn_auc(bert_reps, labels, bert_train)\n",
    "            bert_str  = f'{auc_bert:>10.4f}'\n",
    "        else:\n",
    "            auc_bert = None\n",
    "            bert_str = '      skip'\n",
    "        ssl_ablation_results[(v, ds_name)] = (auc_bi, auc_bert)\n",
    "        print(f'{v:<8}  {ds_name:<15}  {auc_bi:>10.4f}  {bert_str}')\n",
    "    print()\n",
    "\n",
    "# ── Pick best BiMamba variant by CIC AUC ──\n",
    "best_v = max(\n",
    "    [k for k in ssl_ablation_results if k[1] == 'CIC-IDS-2017'],\n",
    "    key=lambda k: ssl_ablation_results[k][0]\n",
    ")[0]\n",
    "print('─' * 50)\n",
    "print(f'Best variant for cross-dataset (CIC) AUC: {best_v.upper()}')\n",
    "\n",
    "# ── Set encoder paths for Phase 3 ──\n",
    "ssl_bimamba_new = all_encoders[best_v][0]\n",
    "ssl_bert_new    = ssl_bert_paper   # BERT paper is the only fully trained BERT\n",
    "\n",
    "SSL_BIMAMBA_PATH_NEW = WEIGHT_DIR / 'phase2_ssl' / f'ssl_bimamba_{best_v}.pth'\n",
    "SSL_BERT_PATH_NEW    = WEIGHT_DIR / 'phase2_ssl' / 'ssl_bert_paper.pth'\n",
    "\n",
    "print(f'Encoder paths for Phase 3 → {SSL_BIMAMBA_PATH_NEW.name}, {SSL_BERT_PATH_NEW.name}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e072333",
   "metadata": {},
   "source": [
    "---\n",
    "## Phase 3: Supervised Teacher Fine-Tuning\n",
    "\n",
    "Load SSL-pretrained encoder weights → freeze nothing → fine-tune end-to-end with cross-entropy on labeled UNSW data.\n",
    "\n",
    "| Model | SSL Init | Epochs | LR | Save Path |\n",
    "|-------|----------|--------|-----|-----------|\n",
    "| BertClassifier | `ssl_bert_paper.pth` → `encoder` | 5 | 1e-4 | `phase3_teachers/bert_teacher.pth` |\n",
    "| BiMambaClassifier | `ssl_bimamba_paper.pth` → `encoder` | 5 | 1e-4 | `phase3_teachers/bimamba_teacher.pth` |\n",
    "\n",
    "**eval on:** UNSW test, CIC-IDS-2017, CTU-13 → Acc / F1 / AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf8aae3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ══════════════════════════════════════════════════════════════════════\n",
    "# CELL 13: Phase 3 — Supervised Teacher Fine-Tuning\n",
    "# ══════════════════════════════════════════════════════════════════════\n",
    "\n",
    "BERT_TEACHER_PATH    = WEIGHT_DIR / 'phase3_teachers' / 'bert_teacher.pth'\n",
    "BIMAMBA_TEACHER_PATH = WEIGHT_DIR / 'phase3_teachers' / 'bimamba_teacher.pth'\n",
    "\n",
    "FT_EPOCHS = 10    # same for BERT and BiMamba (fair comparison)\n",
    "FT_LR     = 1e-4\n",
    "\n",
    "# ── Class weights for 16.7:1 imbalance (94.3% benign, 5.7% attack) ──\n",
    "_ft_labels = train_ds.labels.numpy()\n",
    "_n_benign  = (_ft_labels == 0).sum()\n",
    "_n_attack  = (_ft_labels == 1).sum()\n",
    "CLASS_WEIGHTS = torch.tensor([1.0, float(_n_benign / _n_attack)],\n",
    "                               dtype=torch.float32, device=DEVICE)\n",
    "print(f'  Class weights: benign=1.0, attack={CLASS_WEIGHTS[1]:.1f}  '\n",
    "      f'({_n_benign:,} vs {_n_attack:,})')\n",
    "\n",
    "\n",
    "def finetune_teacher(classifier, ssl_encoder_weights_path, save_path, name):\n",
    "    \"\"\"Load SSL encoder weights into classifier.encoder, then\n",
    "    fine-tune the whole model end-to-end with class-weighted cross-entropy.\"\"\"\n",
    "    if weights_exist(save_path):\n",
    "        load_weights(classifier, save_path)\n",
    "        return classifier.to(DEVICE)\n",
    "\n",
    "    # Load SSL encoder weights into .encoder sub-module (strict=True)\n",
    "    sd = torch.load(ssl_encoder_weights_path, map_location='cpu', weights_only=False)\n",
    "    classifier.encoder.load_state_dict(sd, strict=True)\n",
    "    print(f'  ✓ Loaded SSL encoder from {ssl_encoder_weights_path}')\n",
    "\n",
    "    classifier = classifier.to(DEVICE)\n",
    "    optimizer  = torch.optim.AdamW(classifier.parameters(), lr=FT_LR, weight_decay=1e-4)\n",
    "    scheduler  = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=FT_EPOCHS)\n",
    "    # Class-weighted CE: corrects 16.7:1 benign:attack imbalance\n",
    "    criterion  = nn.CrossEntropyLoss(weight=CLASS_WEIGHTS)\n",
    "\n",
    "    best_val_f1 = 0.0\n",
    "    best_sd     = None\n",
    "\n",
    "    for epoch in range(FT_EPOCHS):\n",
    "        classifier.train()\n",
    "        total_loss = 0; n = 0\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            loss = criterion(classifier(x), y)\n",
    "            optimizer.zero_grad(); loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(classifier.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item(); n += 1\n",
    "        scheduler.step()\n",
    "\n",
    "        acc, f1, auc = evaluate_classifier(classifier, val_loader)\n",
    "        print(f'  Epoch {epoch+1}/{FT_EPOCHS}  loss={total_loss/n:.4f}  '\n",
    "              f'val acc={acc:.4f} f1={f1:.4f} auc={auc:.4f}')\n",
    "        if f1 > best_val_f1:\n",
    "            best_val_f1 = f1\n",
    "            best_sd = copy.deepcopy(classifier.state_dict())\n",
    "\n",
    "    classifier.load_state_dict(best_sd)\n",
    "    save_weights(classifier, save_path)\n",
    "    return classifier\n",
    "\n",
    "\n",
    "# ── Fine-tune BERT Teacher ──\n",
    "print('═' * 60)\n",
    "print(f'Phase 3a: BERT Teacher  ({FT_EPOCHS} epochs, LR={FT_LR})')\n",
    "print('═' * 60)\n",
    "bert_teacher = finetune_teacher(\n",
    "    BertClassifier(), SSL_BERT_PATH_NEW, BERT_TEACHER_PATH, 'BERT')\n",
    "\n",
    "print()\n",
    "\n",
    "# ── Fine-tune BiMamba Teacher ──\n",
    "print('═' * 60)\n",
    "print(f'Phase 3b: BiMamba Teacher  ({FT_EPOCHS} epochs, LR={FT_LR})')\n",
    "print('═' * 60)\n",
    "bimamba_teacher = finetune_teacher(\n",
    "    BiMambaClassifier(), SSL_BIMAMBA_PATH_NEW, BIMAMBA_TEACHER_PATH, 'BiMamba')\n",
    "\n",
    "print()\n",
    "\n",
    "# ── Evaluate on all datasets ──\n",
    "print('Phase 3 Teacher Results\\n')\n",
    "print(f\"{'Model':<18}  {'Dataset':<18}  {'Acc':>7}  {'F1':>7}  {'AUC':>7}\")\n",
    "print('─' * 65)\n",
    "\n",
    "for model_name, model in [('BERT Teacher', bert_teacher), ('BiMamba Teacher', bimamba_teacher)]:\n",
    "    for ds_name, loader in [('UNSW Test', test_loader),\n",
    "                             ('CIC-IDS-2017', cic_loader),\n",
    "                             ('CTU-13', ctu_loader)]:\n",
    "        acc, f1, auc = evaluate_classifier(model.eval(), loader)\n",
    "        print(f'{model_name:<18}  {ds_name:<18}  {acc:>7.4f}  {f1:>7.4f}  {auc:>7.4f}')\n",
    "    print()\n",
    "\n",
    "print('✓ Phase 3 Complete')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mamba_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
