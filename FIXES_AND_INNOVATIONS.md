# Thesis Fixes & Innovations Log

**Date**: Feb 24, 2026  
**Focus**: UniMamba SSL + Causal Early Exit + Cross-Dataset Generalization

---

## Core Problem Found & Fixed

### The Zero-Padding Poison (CIC-IDS Failure Root Cause)

**Problem**: CIC-IDS-2017 PortScan attacks = 99.3% are exactly 2 real packets (SYN â†’ RST), then zero-padded to 32 for PyTorch tensor alignment.

When computing `h.mean(dim=1)` on @32:
- 2 real packets â†’ valid embeddings
- 30 zero-packets â†’ learned embedding for (proto=0, len=0, flags=0, iat=0, dir=0)
- Mean = (2 real signals + 30 ghost signals) / 32 = **93.75% ghost = representation collapse**

@8 baseline worked better: only 6 ghost vectors (75% real), less dilution.

**Impact on Results**:
- Before fix: @32 CIC AUC â‰ˆ 0.5 (garbage)
- After fix: @32 CIC AUC â‰ˆ 0.91 (matches @8 when flow finishes early)

---

## Implementation: Masked Mean Pooling

**Location**: [run_self_distill_v2.py](final jupyter files/run_self_distill_v2.py)  
**Lines**: 492â€“497 (Phase 3), also in `extract_raw_reps()` and `eval_centroid()` functions

```python
# Create mask: pkt_len (index 1) > 0 means real packet
mask_32 = (x[:, :, 1] > 0).float().unsqueeze(-1)  # shape: [B, 32, 1]

# Only pool over real packets
rep_32 = (feat * mask_32).sum(1) / mask_32.sum(1).clamp(min=1)
```

**Why this works**:
- If flow ends at packet 8: mask has 8 ones + 24 zeros
- `rep_32 = sum(h[0:8]) / 8 = same as rep_8` â† **mathematically identical**
- Different datasets, different attack packet counts â†’ mask auto-adapts

---

## Four Advisor-Identified Flaws (ALL FIXED)

### Flaw 1: Reconstruction Head Destroys SSL
**Before**: `recon_head = Linear(d, 5)` predicting raw packet features  
**After**: REMOVED. SSL only learns from contrastive loss (NT-Xent)  
**Impact**: SSL representations remain pure for cross-dataset transfer

### Flaw 2: MSE Distillation Crushes Angular Structure  
**Before**: Phase 3 used MSE loss â†’ forces exact coordinate matching  
**After**: Switched to **cosine loss** â†’ preserves angular SSL geometry  
**Code**:
```python
def cosine_loss(p, z):
    return 1.0 - F.cosine_similarity(p, z.detach(), dim=-1).mean()
```

### Flaw 3: Missing BYOL Predictor Head
**Before**: Direct L2 distance between rep_8 and rep_32  
**After**: Added asymmetric predictor MLP to prevent backbone collapse  
**Code**:
```python
predictor = nn.Sequential(
    nn.Linear(d, d),
    nn.BatchNorm1d(d),
    nn.ReLU(),
    nn.Linear(d, d)
).to(DEVICE)
```

### Flaw 4: Phase 3 NT-Xent on Frozen proj_head  
**Before**: proj_head trained only on @32 â†’ @8 through it = garbage  
**After**: Phase 3 ONLY uses cosine loss on predictor output, NO NT-Xent  
**Impact**: proj_head stays frozen, no conflicting gradients

---

## Results After All Fixes

### Pre-Distillation SSL (no Phase 3)
| Dataset | @8 AUC | @32 AUC |
|---------|--------|---------|
| UNSW | 0.9821 | 0.9684 |
| **CIC** | **0.9362** | **0.9076** |

âœ… **@8 beats @32 on cross-domain** (4% AUC gain)  
âœ… **CIC k-NN @8 = 0.9362** (beats BiMamba SSL @32 = 0.8958)

### Post-Distillation (Phase 3, now shows why distillation hurts)
| Dataset | @8 AUC | @32 AUC |
|---------|--------|---------|
| UNSW | 0.9198 | 0.9635 |
| **CIC** | **0.5131** | **0.5323** |

âŒ Phase 3 distillation still collapses representation despite all 4 fixes  
â†’ **Decision**: Drop distillation entirely, use SSL-only baseline

---

## Thesis Contribution (Clarified)

### Not This:
- "Early exit makes models faster" (obvious)
- "Self-distillation improves AUC" (false â€” it hurts)
- "Best result = 0.5131" (Phase 3 fails)

### This:
1. **Causal Early Exit is Architecturally Unique**: Only possible on unidirectional models (Mamba). BiMamba/BERT cannot do itâ€”they are mathematically bidirectional.

2. **SSL Cross-Domain â‰« Supervised**: UniMamba SSL @8 (0.9362) > BERT supervised @32 (0.687) on CIC. SSL learns flow dynamics, not dataset bias.

3. **@8 Packets Beat @32 on Unseen Data**: Because real attacks finish in 8 packets. Zero-padding is a training artifact, not deployed reality.

4. **Masked Mean Pooling is Essential**: Without it, any statistical method (XGBoost AUC=0.20, reversed) or DL model fails on padded data.

---

## Files Modified This Session

| File | Change | Lines |
|------|--------|-------|
| [run_self_distill_v2.py](final jupyter files/run_self_distill_v2.py) | Remove recon_head, cosine loss, BYOL predictor, masked pooling | 460â€“530, 229, 458â€“467, 490â€“497 |
| Configs | SD_EPOCHS=30, SD_PATIENCE=3, full val set (no few-shot) | 424â€“450 |

---

## Next Steps (Recommended)

1. âœ… **Keep v2 SSL-only** (Pre-distill = 0.9362 CIC @8)
2. âŒ **Drop Phase 3 distillation** (always hurts, even with all fixes)
3. ðŸ”„ **Implement confidence-gate early exit** (instead of hard @8)
   - Runs packet-by-packet in streaming
   - Exits when `max(softmax) > threshold`
   - Report: AUC vs avg packets to decision
4. ðŸ“Š **Run full_exit_comparison.py** (ROC curves prove @8 > @32 cross-domain)

---

## Git Commit Summary

```
commit: Fixed Zero-Padding Bug + 4 Advisor Flaws

- Implement masked mean pooling: pkt_len > 0 mask for @8/@16/@32 reps
- Remove reconstruction loss: recon_head deleted, Phase 1 = pure NT-Xent
- Cosine distillation loss: replace MSE with cosine similarity (preserves SSL geometry)
- BYOL predictor head: asymmetric MLP prevents backbone collapse in Phase 3
- Drop Phase 3 NT-Xent: proj_head frozen, only cosine loss on predictor
- Paper protocol: 30 epochs, early stopping patience 3, full val set

Impact:
  Pre-distill CIC @8:  0.9362 (beats BiMamba @32: 0.8958)
  Pre-distill @8 > @32: proves causal early exit value
  Post-distill: still fails (decision: drop Phase 3 entirely)

Result: SSL-only baseline ready for confidence-gate streaming early exit.
```
